\documentclass{amsart}

\usepackage{amsmath,amsthm,amssymb,bm}
\usepackage{hyperref}
\usepackage{a4wide}
\usepackage{cleveref}
% \usepackage{refcheck}
\usepackage{graphicx,color}
\usepackage{tikz}
\numberwithin{equation}{section}
\usetikzlibrary{snakes}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\theoremstyle{definition}
\newtheorem{exam}[thm]{Example}
\newtheorem{defn}[thm]{Definition}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{question}[thm]{Question}
\newtheorem{problem}[thm]{Problem}
\newtheorem{remark}[thm]{Remark}
\newtheorem*{note}{Note}

% DO NOT DELETE THIS COMMENT!!! MACROS BELOW:
\newcommand\cycle{\operatorname{cycle}}
\newcommand\Motz{\operatorname{Motz}}
\newcommand\Fix{\operatorname{Fix}}
\newcommand\sgn{\operatorname{sgn}}
\newcommand\NN{\mathbb{N}}
\newcommand\QQ{\mathbb{Q}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\RR}{\mathbb{R}}
\newcommand\LL{\mathcal{L}}
\newcommand\FF{\mathbb{F}}
\newcommand\FT{\operatorname{FT}}

\newcommand\Mot{\operatorname{Mot}}
\newcommand{\Dyck}{\operatorname{Dyck}}

\newcommand\Par{\operatorname{Par}}
\newcommand\RPP{\operatorname{RPP}}
\newcommand\SSYT{\operatorname{SSYT}}
\newcommand\SYT{\operatorname{SYT}}

\newcommand\wt{\operatorname{wt}}

\renewcommand\vec[1]{\mathbf{#1}}
\newcommand\vx{\vec{x}}
\newcommand\flr[1]{\left\lfloor #1\right\rfloor}
\newcommand\Qbinom[3]{\genfrac{[}{]}{0pt}{}{#1}{#2}_{#3}}
\newcommand\qbinom[2]{\Qbinom{#1}{#2}{q}}

\newcommand\hyper[5]{{}_{#1}F_{#2} \left(#3;#4;#5\right)}
\newcommand\qhyper[5]{{}_{#1}\phi_{#2} \left(#3;#4;#5\right)}
\newcommand\Hyper[5]{{}_{#1}F_{#2} \left( \left.
    \begin{matrix}
      #3\\
      #4\\
    \end{matrix}
    \:\right|\: #5
    \right)}
\newcommand\qHyper[5]{{}_{#1}\phi_{#2} \left(
    \begin{matrix}
      #3\\
      #4\\
    \end{matrix}
    ; #5
    \right)}

\newcommand\comment[1]{\textcolor{gray}{\bf #1}}
\renewcommand\emph[1]{\textcolor{blue}{\bf #1}}


\def\addable(#1,#2){\draw [blue, line width=1pt] (#1,#2) circle [radius=6pt];}
\def\remove(#1,#2){\fill [red] (#1,#2) circle [radius=4pt];}
\def\BM#1{\draw [line width=2pt] (#1+0.9,0.9) rectangle +(-0.8,-0.8);} 
\def\RM#1{\draw [line width=2pt,red] (#1+0.9,0.9) rectangle +(-0.8,-0.8);}
\def\BD#1{\draw [line width=2pt] (#1+0.9,0.9) rectangle +(-1.8,-0.8);}
\def\RD#1{\draw [line width=2pt,red] (#1+0.9,0.9) rectangle +(-1.8,-0.8);}
\def\LRM#1{\RM{#1} \node at (#1+0.5, 1.3) {$x$};}
\def\LBM#1{\BM{#1} \node at (#1+0.5, 1.3) {$-b_{#1}$};}
\def\LBD#1{\BD{#1} \node at (#1, 1.3) {$-\lambda_{#1}$};}
% \def\LRD#1{\RD{#1} \node at (#1, 1.3) {$-\lambda_{#1}$};}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Lecture notes on the combinatorics of orthogonal polynomials}
\author{Jang Soo Kim}
% \thanks{The author was supported by NRF grants \#2022R1A2C101100911 and \#2016R1A5A1008055.} 
% \address{Department of Mathematics,
% Sungkyunkwan University (SKKU), Suwon, Gyeonggi-do 16419, South Korea}
% \email{jangsookim@skku.edu}
\date{\today}

\begin{document}

% \begin{abstract}
% \end{abstract}


\maketitle
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% start here

\section{Introduction to the lectures}

Orthogonal polynomials are classical objects arising from the study of
continued fractions. Due to the long history of orthogonal
polynomials, they have now become important objects of study in many
areas: classical analysis and PDE, mathematical physics, probability,
random matrix theory, and combinatorics.

The combinatorial study of orthogonal polynomials was pioneered by
Flajolet and Viennot in 1980s. In these lectures we will learn
fascinating combinatorial properties of orthogonal polynomials.

We will first study basic properties of orthogonal polynomials based
on Chihara's book, Chapter~1 \cite{Chihara}. We will then focus on the
combinatorial approach of orthogonal polynomials, which will be based
on Viennot's lecture notes \cite{ViennotLN}. We will also cover more
recent developments in the combinatorics of orthogonal polynomials
such as their connections with ASEP, staircase tableaux, lecture hall
partitions, and orthogonal polynomials of type \( R_1 \).

The prerequisites of this course are Calculus 1, Linear Algebra, and
Discrete Mathematics.

In the first section we study elementary and classical results of
orthogonal polynomials. In the second section we review basics of
enumerative combinatorics. Starting from the third section we focus on
the combinatorics of orthogonal polynomials.

\section{Basics of orthogonal polynomials}
\label{sec:basics-orth-polyn}

In this section we will cover the first chapter of Chihara's book
\cite{Chihara}.

\subsection{Introduction}

Since
\[
  2\cos m\theta \cos n\theta  = \cos(m+n)\theta + \cos(m-n)\theta,
\]
for nonnegative integers \( m \) and \( n \), we have
\begin{equation}\label{eq:coscos=0}
  \int_0^\pi \cos m\theta \cos n\theta d\theta = 0, \qquad m\ne n.
\end{equation}
In this situation we say that \( \cos m\theta \) and
\( \cos n\theta \) are orthogonal over the interval \( (0,\pi) \).

Note that \( \cos n\theta \) is a polynomial in \( \cos\theta \) of
degree \( n \). So we can write \( \cos n\theta = T_n(\cos\theta) \)
for a polynomial \( T_n(x) \) of degree \( x \).

By the change of variable \( x=\cos \theta \), \eqref{eq:coscos=0} can
be rewritten as
\[
  \int_{-1}^1 T_m(x)T_n(x) (1-x^2)^{-1/2} dx = 0, \qquad m\ne n.
\]

The polynomials \( T_n(x) \), \( n\ge0 \), are called the
\emph{Tchebyshev polynomials of the first kind}.
The first few polynomials are:
\begin{align*}
 T_0(x) &= 1, \\
 T_1(x) &= \cos\theta = x, \\
 T_2(x) &= \cos2\theta = 2\cos^2\theta-1 = 2x^2-1, \\
 T_3(x) &= 4x^3-3x.
\end{align*}

Recall that in an inner product space \( V \) with inner product
\( \langle \cdot,\cdot \rangle \), a set of vectors
\( v_1,\dots,v_n \) are said to be orthogonal if
\( \langle v_i,v_j \rangle = 0 \) for all \( i\ne j \). In this sense
the Tchebyshev polynomials \( T_n(x) \) are orthogonal, where
\( V = \RR[x] \) is the space of polynomials with real coefficients
with the inner product given by
\[
  \langle f(x), g(x) \rangle = \int_{-1}^1 f(x)g(x) (1-x^2)^{-1/2} dx.
\]
We say that \( T_n(x) \) are \emph{orthogonal polynomials} with
respect to the \emph{weight function} \( (1-x^2)^{-1/2} \) on the
interval \( (-1,1) \).

\begin{defn}\label{def:OPS1}
  Suppose that \( w(x) \) is a nonnegative and integrable function on
  \( (a,b) \) with \( \int_a^b w(x)dx >0 \) and
  \( \int_a^b x^n dx < \infty \) for all \( n\ge0 \). A sequence of
  polynomials \( \{P_n(x)\}_{n\ge0} \) is called an \emph{orthogonal
    polynomial sequence (OPS)} with respect to the \emph{weight
    function} (or \emph{measure}) \( w(x) \) on \( (a,b) \) if the
  following conditions hold:
  \begin{enumerate}
  \item \( \deg P_n(x) = n \), for \( n\ge0 \),
  \item \( \int_a^b P_m(x)P_n(x) w(x)dx = 0 \) for \( m\ne n \).
  \end{enumerate}
\end{defn}

There is another way to define orthogonal polynomials without using
the weight function. For a polynomial \( f(x) \), if we define
\[
  \LL(f(x)) = \int_a^b f(x) w(x)dx,
\]
then \( \LL(f(x)) \) is completely determined by the \emph{moments}
\( \mu_n = \int_a^b x^n w(x)dx \). So, if we are only interested in
polynomials, then we can define a linear functional \( \LL \) using a
moment sequence \( \mu_0,\mu_1,\dots \). Not every sequence
\( \mu_0,\mu_1,\dots \) gives rise to an OPS, though. We will see
later a criterion for a sequence to be a moment sequence.

\begin{defn}\label{def:OPS2}
  Let \( \LL \) be a linear functional defined on the space of
  polynomials in \( x \). A sequence of polynomials
  \( \{P_n(x)\}_{n\ge0} \) is called an \emph{orthogonal polynomial
    sequence (OPS)} with respect to \( \LL \) if the following
  conditions hold:
  \begin{enumerate}
  \item \( \deg P_n(x) = n \), \( n\ge0 \),
  \item \( \LL(P_m(x)^2) \ne 0 \) for \( m\ge0 \),
  \item \( \LL(P_m(x)P_n(x))  = 0 \) for \( m\ne n \).
  \end{enumerate}
\end{defn}

Note that the second condition above was not necessary in
\Cref{def:OPS1} because it follows from the facts that \( w(x) \) is
nonnegative and \( \int_a^b w(x)dx >0 \).

\begin{remark}
  The moments of the Tchebyshev polynomials are
  \[
    \mu_{2n} = \int_{-1}^1 x^{2n} (1-x^2)^{-1/2} dx
    = \frac{\pi}{2^{2n}} \binom{2n}{n}, \qquad
    \mu_{2n+1} = 0.
  \]
  This suggests that there could be some interesting combinatorics
  behind the scene. We will later find a combinatorial way to
  understand this situation.
\end{remark}


\begin{exam}[Charlier polynomials]
  The \emph{Charlier polynomials} \( P_n(x) \) are defined by
  \[
    P_n(x) = \sum_{k=0}^{n} \binom{x}{k} \frac{(-a)^{n-k}}{(n-k)!},
  \]
  where \( \binom{x}{k} = x(x-1)\cdots(x-k+1)/k! \).
  We will find a different type of orthogonality for \( P_n(x) \).

  The generating function for \( P_n(x) \) is
  \[
    G(x,w) = \sum_{n\ge0} P_n(x) w^n
    = \sum_{n\ge0} \left( \sum_{k=0}^{n} \binom{x}{k} \frac{(-a)^{n-k}}{(n-k)!} \right) w^n
    = \sum_{n\ge0} \binom{x}{n} w^n \sum_{n\ge 0} \frac{(-a)^m}{m!} w^m,
  \]
  which means
  \[
    G(x,w) =  e^{-aw}(1+w)^x .
  \]
  Thus
  \[
    a^x G(x,v) G(x,w) = e^{-a(v+w)} \left( a(1+v)(1+w) \right)^x .
  \]

  We have
  \[
    \sum_{k\ge 0} \frac{a^k G(k,v)G(k,w)}{k!}
    = \sum_{k\ge 0} \frac{e^{-a(v+w)} \left( a(1+v)(1+w) \right)^k}{k!} 
    = e^{-a(v+w)} e^{a(1+v)(1+w)} = e^ae^{avw}.
  \]
  Thus
  \begin{equation}\label{eq:G(k,v)G(k,w)}
    \sum_{k\ge 0} \frac{a^k G(k,v)G(k,w)}{k!}
    = \sum_{n\ge 0} \frac{e^a (avw)^n}{n!}.
  \end{equation}

On the other hand
\begin{align}
  \notag
    \sum_{k\ge 0} \frac{a^k G(k,v)G(k,w)}{k!}
    &= \sum_{k\ge 0} \frac{a^k}{k!} \sum_{m,n\ge0} P_m(k) P_n(k) v^m w^n\\
  \label{eq:PPvw}
    &= \sum_{m,n\ge0} \left( \sum_{k\ge 0}  P_m(k) P_n(k) \frac{a^k}{k!} \right)v^m w^n.
\end{align}
Comparing the coefficients of \( v^mw^n \) in \eqref{eq:G(k,v)G(k,w)} and \eqref{eq:PPvw} we obtain
\begin{equation}\label{eq:charlier-orthogonality}
  \sum_{k\ge 0}  P_m(k) P_n(k) \frac{a^k}{k!} = \frac{e^a a^n}{n!} \delta_{n,m}.
\end{equation}
Therefore, if we define a linear functional \( \LL \) by
\[
 \LL(x^n)  = \sum_{k\ge 0} k^n \frac{a^k}{k!},
\]
then \( P_n(x) \) are orthogonal polynomials with respect to \( \LL \).

Note that we describe the orthogonality of \( P_n(x) \) using only the
linear functional \( \LL \) without referring to any weight function.
However, we can also find a weight function in this case. Let
\( \psi(x) \) be the step function with a jump at \( k=0,1,2,\ldots \) of
magnitude \( a^k/k! \).
Then the linear functional \( \LL \) can be written as the following
Riemann--Stieltjes integral
\[
  \LL(f(x)) = \int_{-\infty}^\infty f(x) d \psi(x).
\]
\end{exam}

We can also prove \eqref{eq:charlier-orthogonality} in a combinatorial
way, see \Cref{sec:sign-revers-invol}.




\begin{remark}
  In the theory of orthogonal polynomials, finding an explicit weight
  function is an important problem. However, in these lectures, we
  will not pursue in this direction and we will be mostly satisfied
  with \Cref{def:OPS2}.
\end{remark}


\subsection{The moment functional and orthogonality}

We will consider the space \( \CC[x] \) of polynomials with complex
coefficients. A \emph{linear functional} on \( \CC[x] \) is a map
\( \LL:\CC[x] \to \CC \) such that
\( \LL(af(x)+bg(x)) = a\LL(f(x)) + b\LL(g(x)) \) for all
\( f(x), g(x)\in \CC[x] \) and \( a,b\in \CC \).

\begin{defn}
  Let \( \{\mu_{n}\}_{n\ge0} \) be a sequence of complex numbers. Let
  \( \LL \) be the linear functional on the space of polynomials
  defined by \( \LL(x^n) = \mu_n \), \( n\ge0 \). In this case we say
  that \( \LL \) is the \emph{moment functional} determined by the
  \emph{moment sequence} \( \{\mu_n\} \), and \( \mu_n \) is called
  the \emph{\( n \)th moment}.
\end{defn}

We recall the definition of orthogonal polynomials.

\begin{defn}
  Let \( \LL \) be the linear functional defined on the space of
  polynomials in \( x \). A sequence of polynomials
  \( \{P_n(x)\}_{n\ge0} \) is called an \emph{orthogonal polynomial
    sequence (OPS)} with respect to \( \LL \) if the following
  conditions hold:
  \begin{enumerate}
  \item \( \deg P_n(x) = n \), \( n\ge0 \),
  \item \( \LL(P_m(x)P_n(x))  = K_n \delta_{m,n} \), for some \( K_n\ne 0 \).
  \end{enumerate}
\end{defn}

We say that \( P_n(x) \) are \emph{orthonormal} if
\( \LL(P_m(x)P_n(x)) = \delta_{m,n} \).


\begin{thm}\label{thm:orth-equiv}
  Let \( \{P_n(x) \} \) be a sequence of polynomials and let \( \LL \) be
  a linear functional. The following are equivalent:
  \begin{enumerate}
  \item \( \{P_n(x) \} \) is an OPS with respect to \( \LL \);
  \item \( \LL(\pi(x) P_n(x)) = 0 \) if \( \deg \pi(x) < n \) and
    \( \LL(\pi(x) P_n(x)) \ne 0 \) if \( \deg \pi(x) = n \);
  \item \( \LL(x^m P_n(x)) = K_n \delta_{m,n} \), \( 0\le m\le n \), for some \( K_n\ne 0 \).
  \end{enumerate}
\end{thm}
\begin{proof}
\( (1) \Rightarrow (2) \):
Suppose that \( \deg \pi(x) \le n \).
Since \( \{P_n(x) \} \) is a basis of \( \CC[x] \), we can write
\[
  \pi(x) = c_0 + c_1 P_1(x) + \cdots + c_n P_n(x).
\]
Then
\[
  \LL(\pi(x) P_n(x)) = \sum_{k=0}^n \LL \left( c_k P_k(x) P_n(x) \right)
  = c_n \LL(P_n(x)^2),
\]
which is zero if \( \deg \pi(x) <n \) and nonzero if \( \deg \pi(x) =n \).

\( (2) \Rightarrow (3) \): Trivial.
\( (2) \Rightarrow (3) \): Trivial.
\end{proof}

\begin{thm}\label{thm:orth-coeff}
  Suppose that \( \{ P_n(x) \}_{n\ge 0} \) be an OPS with respect to \( \LL \).
  Then for any polynomial \( \pi(x) \) of degree \( n \),
\[
  \pi(x) = \sum_{k=0}^n  c_k P_k(x), \qquad
  c_k = \frac{\LL(\pi(x)P_k(x))}{\LL(P_k(x)^2)}.
\]
\end{thm}
\begin{proof}
  Clearly, we can write
  \[
    \pi(x) = \sum_{k=0}^n c_k P_k(x),
  \]
  for some \( c_k \). Multiplying \( P_j(x) \) both sides
  and taking \( \LL \), we get
  \[
    \LL(\pi(x) P_j(x)) = \sum_{k=0}^n \LL \left( c_k P_k(x) P_j(x) \right)
    = c_j \LL(P_j(x)^2).
  \]
  Dividing both sides by \( \LL(P_j(x)^2) \), we obtain the theorem.
\end{proof}

\begin{thm}\label{thm:uniqueness-OPS}
  Suppose that \( \{ P_n(x) \}_{n\ge 0} \) be an OPS with respect to
  \( \LL \). Then \( P_n(x) \) is uniquely determined by \( \LL \) up
  to a nonzero factor. More precisely, if \( \{ Q_n(x) \}_{n\ge 0} \)
  is an OPS with respect to \( \LL \), then there are constants
  \( c_n\ne0 \) such that \( Q_n(x) = c_n P_n(x) \) for all
  \( n\ge0 \).
\end{thm}
\begin{proof}
  Let us write \(Q_n(x) = \sum_{k=0}^n c_k P_k(x) \). Then by
  \Cref{thm:orth-coeff}, \( c_k = \LL(Q_n(x)P_k(x))/\LL(P_k(x)^2) \).
  But by \Cref{thm:orth-equiv}, \( \LL(Q_n(x)P_k(x)) = 0 \) unless
  \( k=n \). Thus \( Q_n(x) = c_n P_n(x) \).
\end{proof}


Note that if \( \{ P_n(x) \}_{n\ge 0} \) is an OPS for \( \LL \), then
so is \( \{ c_nP_n(x) \}_{n\ge 0} \) for any \( c_n\ne 0 \). Therefore
there is a unique monic OPS, which is obtained by dividing each
\( P_n(x) \) by its leading coefficient. Note also that there is a
unique orthonormal OPS as well given by
\( p_n(x) = P_n(x)/\LL(P_n(x)^2)^{1/2} \). In summary we have the following
corollary.

\begin{cor}\label{cor:OPS-unique}
  Suppose that \( \LL \) is a moment sequence such that there is an
  OPS for \( \LL \). Let \( K_n \), \( n\ge0 \), be a sequence of
  nonzero numbers. Then the following hold.
  \begin{enumerate}
  \item There is a unique monic OPS \( \{ P_n(x) \}_{n\ge 0} \) for \( \LL \).
  \item There is a unique OPS \( \{ P_n(x) \}_{n\ge 0} \) for \( \LL \)
    such that the leading coefficient of \( P_n(x) \) is \( K_n \).
  \item There is a unique OPS \( \{ P_n(x) \}_{n\ge 0} \) for \( \LL \)
    such that \( \LL(x^nP_n(x)) = K_n \).
  \end{enumerate}
\end{cor}


Clearly, if \( \{ P_n(x) \}_{n\ge 0} \) is an OPS for \( \LL \), then
it is also an OPS for \( \LL' \) given by
\( \LL'(f(x)) = c \LL(f(x)) \) for some \( c\ne 0 \). Therefore, by
dividing the linear functional by the value \( \LL(1) \), we may
assume that \( \LL(1)=1 \).


\subsection{Existence of OPS}

The main question in this section is: for what linear functional
\( \LL \) does there exist an OPS? To answer this question we need the
following definition.

\begin{defn}
  The \emph{Hankel determinant} of a moment sequence \( \{\mu_n\} \)
  is defined by
  \[
    \Delta_n = \det(\mu_{i+j})_{i,j=0}^n
    = \begin{vmatrix}
        \mu_0 & \mu_1 & \cdots & \mu_n\\
        \mu_1 & \mu_2 & \cdots & \mu_{n+1}\\
        \vdots & \vdots & \ddots & \vdots\\
        \mu_n & \mu_{n+1} & \cdots & \mu_{2n}
      \end{vmatrix} .
  \]
\end{defn}

\begin{thm}\label{thm:Dne0}
  Let \( \LL \) be a linear functional with moment sequence
  \( \{\mu_n\} \).
  Then there is an OPS for \( \LL \) if and only if
  \( \Delta_n\ne 0 \) for all \( n\ge0 \).
\end{thm}

\begin{proof}
  Fix a sequence \( \{K_n\} \) of nonzero real numbers \( K_n \). By
  \Cref{cor:OPS-unique}, if there is an OPS
  \( \{ P_n(x) \}_{n\ge 0} \) for \( \LL \), it is uniquely determined
  by the condition \( \LL(x^n P_n(x)) = K_n \), \( n\ge0 \). In other
  words, using \Cref{thm:orth-equiv}, there is an OPS for \( \LL \) if
  and only if there is a unique sequence \( \{ P_n(x) \}_{n\ge 0} \)
  of polynomials such that
  \begin{equation}\label{eq:1}
    \LL(x^mP_n(x)) = K_n \delta_{m,n}, \qquad 0\le m\le n.
  \end{equation}
  
  Now let \( P_n(x) = \sum_{k=0}^{n} c_{n,k} x^k \). Multiplying both
  sides by \( x^m \) and taking \( \LL \), we get
  \[
    \LL(x^mP_n(x)) = \sum_{k=0}^{n} c_{n,k} \mu_{n+k}.
  \]
  Thus (\ref{eq:1}) can be written as the matrix equation
  \begin{equation}\label{eq:2}
   \begin{pmatrix}
     \mu_0 & \mu_1 & \cdots & \mu_n\\
     \mu_1 & \mu_2 & \cdots & \mu_{n+1}\\
     \vdots & \vdots & \ddots & \vdots\\
     \mu_n & \mu_{n+1} & \cdots & \mu_{2n}
   \end{pmatrix}
\begin{pmatrix}
c_{n,0} \\ c_{n,1} \\ \vdots \\ c_{n,n}
\end{pmatrix} 
= \begin{pmatrix}
0 \\ \vdots \\ 0 \\ K_n
\end{pmatrix}. 
  \end{equation}
  Then the uniqueness of the polynomials \( P_n(x) \) satisfying
  (\ref{eq:1}) is equivalent to the uniqueness of the solution of the
  matrix equation (\ref{eq:2}) in \( c_{n,0},c_{n,1},\dots,c_{n,n} \).
  In order for (\ref{eq:2}) to have a unique solution, the Hankel
  determinant \( \Delta_n \) must be nonzero for all \( n\ge0 \).
  Moreover, by Cramer's rule, \( c_{n,n} = K_n\Delta_n/\Delta_{n-1} \)
  is nonzero iff \( \Delta_n\ne 0 \). This proves the theorem.
\end{proof}

Applying Cramer's rule to \eqref{eq:2} we can prove the following
lemma, which will be used later.

\begin{lem}\label{lem:L(pi*P)}
  Let \( \{ P_n(x) \}_{n\ge 0} \) be an OPS for \( \LL \).
  Then for a polynomial \( \pi(x) \) of degree \( n \) we have
 \[
  \LL(\pi(x)P_n(x)) = \frac{ab\Delta_n}{\Delta_{n-1}},
\] 
where \( a \) and \( b \) are the leading coefficients of \( \pi(x) \)
and \( P_n(x) \), respectively. In particular, if
\( \{ P_n(x) \}_{n\ge 0} \) is the monic OPS for \( \LL \), then
\[
  \LL(P_n(x)^2) = \frac{\Delta_n}{\Delta_{n-1}}.
\]
\end{lem}
\begin{proof}
  We use the notation in the proof of \Cref{thm:Dne0}. By solving
  \eqref{eq:2} using Cramer's rule, we obtain that the leading
  coefficient of \( P_n(x) \) is
  \( b= c_{n,n} = K_n \Delta_{n-1}/\Delta_n \). Thus, if we let
  \( \pi(x) = \sum_{k=0}^n a_k x^k \), we have
\[
  \LL(\pi(x)P_n(x)) = \sum_{k=0}^n \LL(a_{k}x^kP_n(x))
  = a_{n}\LL(x^n P_n(x)) = a K_n = \frac{ab\Delta_n}{\Delta_{n-1}},
\]
as desired.
\end{proof}

Similarly every coefficient \( c_{n,i} \) of \( P_n(x) \) can be
computed using \eqref{eq:2}. Thus we have an explicit determinant
formula for \( P_n(x) \).

\begin{thm}\label{thm:P=Hankel}
  Let \( \LL \) be a linear functional with moment sequence
  \( \{\mu_n\} \) with \( \Delta_n\ne 0 \) for all \( n\ge0 \).
  Then the monic OPS for \( \LL \) is given by
  \[
    P_n(x) = \frac{1}{\Delta_{n-1}}
    \begin{vmatrix}
      \mu_0 & \mu_1 & \cdots & \mu_n\\
      \mu_1 & \mu_2 & \cdots & \mu_{n+1}\\
      \vdots & \vdots & \ddots & \vdots\\
      \mu_{n-1} & \mu_{n} & \cdots & \mu_{2n-1}\\
      1 & x & \cdots & x^n
    \end{vmatrix}.
  \]
\end{thm}

\begin{proof}
  This can be proved using \eqref{eq:2}. We can also prove directly
  that \( \{ P_n(x) \}_{n\ge 0} \) satisfies the conditions for an
  OPS. First, the coefficient of \( x^n \) in \( P_n(x) \) is
  \( 1 \), so \( \deg P_n(x) = n \). For
  \( 0\le k\le n \), we have
  \[
    \LL(x^k P_n(x))
    = \frac{1}{\Delta_{n-1}} \LL \left(
      \begin{vmatrix}
        \mu_0 & \mu_1 & \cdots & \mu_n\\
        \mu_1 & \mu_2 & \cdots & \mu_{n+1}\\
        \vdots & \vdots & \ddots & \vdots\\
        \mu_{n-1} & \mu_{n} & \cdots & \mu_{2n-1}\\
        x^{k} & x^{k+1} & \cdots & x^{n+k}
      \end{vmatrix}
    \right) = \frac{1}{\Delta_{n-1}}
    \begin{vmatrix}
        \mu_0 & \mu_1 & \cdots & \mu_n\\
        \mu_1 & \mu_2 & \cdots & \mu_{n+1}\\
        \vdots & \vdots & \ddots & \vdots\\
        \mu_{n-1} & \mu_{n} & \cdots & \mu_{2n-1}\\
        \mu_k & \mu_{k+1} & \cdots & \mu_{n+k}
      \end{vmatrix}.
  \]
  If \( k<n \), then the right-hand side of the above equation has two
  identical rows, hence zero. If \( k=n \), the right-hand side is
  \( \Delta_n/\Delta_{n-1} \ne 0 \). This implies that
  \( \{ P_n(x) \}_{n\ge 0} \) is an OPS for \( \LL \).
\end{proof}

In many important cases of orthogonal polynomials there is a
nonnegative weight function \( w(x) \) representing the moment
functional: \( \LL(x^n) = \int_a^b x^n w(x) dx \). In more general
cases, \( \LL \) can be represented using the Riemann--Stieltjes
integral \( \LL(x^n) = \int_a^b x^n d\psi (x) \), where \( \psi(x) \)
is a nondecreasing function such that
\( \{x: \psi(x+\epsilon)-\psi(x-\epsilon)>0 \mbox{ for all
  \( \epsilon>0 \)} \} \) is an infinite set. It is known
\cite[Chapter~2]{Chihara} that there is such an expression if and only
if \( \LL(\pi(x))>0 \) for all nonzero polynomials \( \pi(x) \) such
that \( \pi(x)\ge0 \) for all \( x\in\RR \).

A \emph{nonnegative-valued} polynomial is a polynomial \( \pi(x) \)
such that \( \pi(x)\ge 0 \) for all \( x\in \RR \).

\begin{defn}
  A linear functional \( \LL \) is \emph{positive-definite} if
  \( \LL(\pi(x))>0 \) for all nonzero nonnegative-valued polynomials
  \( \pi(x) \).
\end{defn}

If \( \LL \) is positive-definite, then it has a real OPS. We will see
later that the converse is not true.

\begin{thm}\label{thm:pos-def-ops}
  Let \( \LL \) be a positive-definite linear functional. Then
  \( \LL \) has real moments and there is a real OPS for \( \LL \).
\end{thm}

\begin{proof}
  First, we show that the moments \( \mu_n \) are real. Since
  \( \LL \) is positive-definite, \( \mu_{2n} = \LL(x^{2n}) >0 \) is
  real. Since
  \( \LL((x+1)^{2n}) = \sum_{k=0}^{2n}\binom{2n}{k} \mu_{k} \) is
  real, by induction, we obtain that \( \mu_{2n-1} \) is also real.

  Now, we construct a real OPS \( \{ P_n(x) \}_{n\ge 0} \) for
  \( \LL \). Let \( P_0(x) = 1 \). Suppose that we have constructed
  real polynomials \( P_0,\dots,P_n \) which are orthogonal with
  respect to \( \LL \), i.e., for \( 0\le i,j\le n \),
  \( \LL(P_i(x)P_j(x)) \) is zero if \( i\ne j \) and nonzero if
  \( i=j \). Now we need to find
  \begin{equation}\label{eq:3}
    P_{n+1} (x) = x^{n+1} + \sum_{k=0}^n a_k P_k(x)
  \end{equation}
  such that \( \LL(P_k(x)P_{n+1}(x)) = 0 \) for all \( 0\le k\le n \).
  Multiplying \( P_k(x) \) and taking \( \LL \) in \eqref{eq:3} we get
  \( \LL(P_k(x)P_{n+1}(x)) = \LL(x^{n+1}P_k(x))+a_k\LL(P_k(x)^2) \).
  Thus, if we set
  \[
    a_k = - \frac{\LL(x^{n+1}P_k(x))}{\LL(P_k(x)^2)},
  \]
  which is real, then \( P_{n+1}(x) \) is orthogonal to
  \( P_0(x),\dots,P_n(x) \). In this way we can construct a real OPS
  \( \{ P_n(x) \}_{n\ge 0} \) for \( \LL \).
\end{proof}

Note that if \( \LL \) is positive-definite, then
\( \LL(P_n(x)^2)>0 \). Thus in this case we can construct a real
orthonormal OPS \( \{ p_n(x) \}_{n\ge 0} \) by rescaling:
\( p_n(x) = P_n(x)/\sqrt{\LL(P_n(x)^2)} \).

Nonnegative-valued polynomials have the following useful property.

\begin{lem}\label{lem:pi=p2+q2}
  Let \( \pi(x) \) be a nonnegative-valued polynomial. Then
  \( \pi(x) = p(x)^2 + q(x)^2 \) for some real polynomials \( p(x) \)
  and \( q(x) \).
\end{lem}
\begin{proof}
  Since \( \pi(x) \) is real for all real \( x \), the coefficients of
  \( \pi(x) \) are real. This can be seen inductively by observing
  that if \( \deg \pi(x) =n \), then the leading coefficient of
  \( \pi(x) \) is equal to
  \[
    \lim_{x\to \infty} \frac{\pi(x)}{x^n}.
  \]
  Since \( \pi(x) \) is a real polynomial such that \( \pi(x)\ge0 \),
  every real zero of \( \pi(x) \) has even multiplicity and complex
  roots appear in conjugate pairs. Thus we can write
  \[
    \pi(x) = r(x)^2 \prod_{k=1}^{m} (x-\alpha_k-\beta_ki)(x-\alpha_k+\beta_ki),
  \]
  where \( r(x) \) is a real polynomial and
  \( \alpha_k,\beta_k\in \RR \).
  If we write \( \prod_{k=1}^{m} (x-\alpha_k-\beta_ki) = A(x)+iB(x) \),
  then \( \prod_{k=1}^{m} (x-\alpha_k+\beta_ki) = A(x)-iB(x) \).
  Thus \( \pi(x) = r(x)^2(A(x)^2+B(x)^2) \) as desired.
\end{proof}

By \Cref{lem:pi=p2+q2}, we have the following criterion for linear
functionals.
\begin{cor}\label{cor:pos-def-sq}
  A linear functional \( \LL \) is positive-definite if and only if
  \( \LL(p(x)^2)>0 \) for every nonzero real polynomial \( p(x) \).
\end{cor}



You may wonder why \( \LL \) is called ``positive-definite''. To see
this recall that a real \( n\times n \) matrix \( A \) is positive
definite if \( u^T A u >0 \) for every nonzero vector
\( u\in \RR^n \). Sylvester's criterion says that \( A \) is positive
definite if and only if every principal minor of \( A \) is positive.
The following theorem justifies the terminology ``positive-definite''
for \( \LL \).


\begin{thm}\label{thm:pos-def-equiv2}
  A linear functional \( \LL \) is positive-definite if and only if
  every moment \( \mu_n \) is real and \( \Delta_n>0 \) for all
  \( n\ge0 \). In other words, \( \LL \) is positive-definite if and
  only if the Hankel matrix \( (\mu_{i+j})_{i,j=0}^n \) is
  positive-definite for all \( n\ge0 \).
\end{thm}
\begin{proof}
  (\(\Rightarrow\)) By \Cref{thm:pos-def-ops}, the moments are real
  and there is a real OPS \( \{ P_n(x) \}_{n\ge 0} \) for \( \LL \).
  By \Cref{lem:L(pi*P)}, \( \Delta_n/\Delta_{n-1} = \LL(P_n(x)^2)>0 \)
  for \( n\ge0 \), where \( \Delta_{-1}=1 \).
  Thus by induction we obtain \( \Delta_n>0 \) for all \( n\ge0 \).

  (\(\Leftarrow\)) Since \( \Delta_n>0 \), by \Cref{thm:Dne0}, there
  is an OPS \( \{ P_n(x) \}_{n\ge 0} \) for \( \LL \). By
  \Cref{cor:pos-def-sq}, it suffices to show that
  \( \LL(p(x)^2) > 0 \) for any nonzero real polynomial \( p(x) \). To
  do this let \( p(x) = \sum_{k=0}^n a_k P_k(x) \). Then by the
  orthogonality,
  \[
    \LL(p(x)^2) = \sum_{k=0}^n a_k^2 \LL(P_k(x)^2).
  \]
  Since \( \Delta_n>0 \), we have \( \LL(P_k(x)^2)>0 \) by
  \Cref{lem:L(pi*P)}. Thus \( \LL(p(x)^2)>0 \) as desired.
\end{proof}


\subsection{The three-term recurrence}

One important property of orthogonal polynomials is that they satisfy
a 3-term recurrence relation.

\begin{thm}\label{thm:3-RR}
  Let \( \LL \) be a linear functional with monic OPS
  \( \{ P_n(x) \}_{n\ge 0} \). Then these monic orthogonal polynomials
  satisfy the following 3-term recurrence relation:
  \begin{equation}\label{eq:3-rr}
    P_{n+1}(x) = (x-b_n) P_n(x) - \lambda_n P_{n-1}(x), \qquad n\ge0,
  \end{equation}
  with initial conditions \( P_{-1}(x) = 0 \) and \( P_0(x) = 1 \) for
  some sequences \( \{b_n\}_{n\ge0} \) and \( \{\lambda_n\}_{n\ge1} \)
  such that \( \lambda_n\ne 0 \).
\end{thm}
\begin{proof}
  Since \( P_n(x) \) are monic polynomials, \( P_{n+1}(x) - xP_n(x) \)
  has degree at most \( n \). Thus we can write
  \[
    P_{n+1}(x) - xP_n(x) = \sum_{k=0}^n a_k P_k(x).
  \]
  By \Cref{thm:orth-equiv}, multiplying both sides by \( P_j(x) \) for \( 0\le j\le n-2 \)
  and taking \( \LL \) gives
  \[
 0 = \LL(P_j(x) P_{n+1}(x) - xP_j(x)P_n(x))
    = \sum_{k=0}^n a_k \LL(P_j(x) P_k(x))  = a_j \LL(P_j(x)^2).
  \]
  Since \( \LL(P_j(x)^2) \ne 0 \), we obtain \( a_j=0 \) for all
  \( 0\le j\le n-2 \). Then we alway have
  \( P_{n+1}(x) - xP_n(x) = a_nP_n(x) + a_{n-1}P_{n-1}(x) \) for some
  constants \( a_n \) and \( a_{n-1} \). This implies that the
  polynomials \( P_n(x) \) satisfy the 3-term recurrence relation
  \eqref{eq:3-rr}.

  It remains to show that \( \lambda_n\ne 0 \). Multiplying
  \( x^{n-1} \) both sides of \eqref{eq:3-rr} and taking \( \LL \)
  gives
  \begin{equation}\label{eq:5}
    0 = \LL(x^{n-1} P_{n+1}(x)) = \LL(x^nP_n(x)) - b_n \LL(x^{n-1}
    P_n(x)) - \lambda_n \LL(x^{n-1}P_{n-1}(x)).
  \end{equation}
  By \Cref{lem:L(pi*P)}, we have
  \( \LL(x^n P_n(x)) = \LL(P_n(x) P_n(x)) \). Thus \eqref{eq:5}
  implies
  \begin{equation}\label{eq:14}
  \lambda_n = \frac{\LL(P_n(x)^2)}{\LL(P_{n-1}(x)^2)}.
  \end{equation}
  Since \( \LL(P_n(x)^2)\ne0 \), we get \( \lambda_n\ne0 \).
\end{proof}

\begin{thm}
  Following the notation in \Cref{thm:3-RR}, we have
  \begin{align}
    \label{eq:la=DD/D}
    \lambda_n &= \frac{\LL(P_n(x)^2)}{\LL(P_{n-1}(x)^2)} = \frac{\Delta_{n-2}\Delta_n}{\Delta_{n-1}^2}, \\
    \label{eq:b=xP/P}
    b_n &= \frac{\LL(xP_{n}(x)^2)}{\LL(P_{n}(x)^2)},\\
    \label{eq:P2=lala}
    \LL(P_n(x)^2) &= \lambda_1\cdots\lambda_{n} \LL(1) = \frac{\Delta_n}{\Delta_{n-1}}, \\
    \label{eq:D=lala}
    \Delta_n &= \lambda_1^{n}\lambda_2^{n-1}\cdots\lambda_{n}^1 \LL(1)^{n+1}.
  \end{align}
\end{thm}

\begin{proof}
  By \Cref{lem:L(pi*P)}, we have
  \( \LL(P_n(x)^2) = \Delta_n/\Delta_{n-1} \). Thus the first identity
  \eqref{eq:la=DD/D} follows from \eqref{eq:14}.

  Multiplying \( P_n(x) \) both sides of \eqref{eq:3-rr} and taking
  \( \LL \) gives
  \begin{align*}
    0 &= \LL(P_n(x)P_{n+1}(x))
       = \LL(xP_n(x)^2)- b_n\LL(P_n(x)^2) - \lambda_n \LL(P_nP_{n-1}(x))\\
    &= \LL(xP_n(x)^2)- b_n\LL(P_n(x)^2),
  \end{align*}
  which implies \eqref{eq:b=xP/P}.

  The identity \eqref{eq:P2=lala} is an immediate consequence of
  \eqref{eq:la=DD/D}. The identity \eqref{eq:D=lala} follows from
  \eqref{eq:P2=lala}.
\end{proof}

\begin{cor}\label{cor:la-pos-def}
  Following the notation in \Cref{thm:3-RR}, the linear functional
  \( \LL \) is positive-definite if and only if \( b_n\in \RR \) and
  \( \lambda_n>0 \) for all \( n \) and \( \LL(1)>0 \).
\end{cor}
\begin{proof}
  Suppose that \( \LL \) is positive-definite. Then by
  \Cref{thm:pos-def-ops} the polynomials \( P_n(x) \) are real, hence
  the recurrence coefficients \( b_n \) and \( \lambda_n \) are real.
  By \Cref{thm:pos-def-equiv2}, we have \( \Delta_n>0 \), which
  together with \eqref{eq:la=DD/D} implies \( \lambda_n>0 \).

  Now suppose that \( b_n\in \RR \) and \( \lambda_n>0 \) for all
  \( n \). By \eqref{eq:la=DD/D} and \eqref{eq:b=xP/P}, one can easily
  check by induction that all the moments are real. By \eqref{eq:D=lala},
  we have \( \Delta_n>0 \). Thus by \Cref{thm:pos-def-equiv2},
  \( \LL \) is positive-definite.
\end{proof}

Oftentimes non-monic orthogonal polynomials are used in the
literature. We can always make them monic by dividing each polynomial
by its leading coefficient. This allows us to convert a 3-term
recurrence of monic orthogonal polynomials to that of non-monic
orthogonal polynomials and vice versa.

Suppose that \( \{ P_n(x) \}_{n\ge 0} \) is an OPS for \( \LL \),
which is not monic. If \( k_n \) is the leading coefficient of
\( P_n(x) \), then the monic OPS for \( \LL \) is given by
\( \{ \hat{p}_n(x) \}_{n\ge 0} \), where
\( \hat{p}_n(x) = P_n(x)/k_n \).
Then, by \Cref{thm:3-RR}, we have
\begin{equation}\label{eq:3-rr-monic}
  \hat{p}_{n+1}(x) = (x-b_n) \hat{p}_n(x) - \lambda_n
  \hat{p}_{n-1}(x), \quad n\ge0; \quad \hat{p}_{-1}(x) = 0, \hat{p}_{0}(x) = 1.
\end{equation}
Substituting \( \hat{p}_n(x) = P_n(x)/k_n \) in the above formula, we get
\begin{equation}\label{eq:3-rr-nonmonic}
  P_{n+1}(x) = (A_nx-B_n) P_n(x) - C_n P_{n-1}(x), \quad n\ge0; \quad P_{-1}(x) = 0, P_{0}(x) = k_0,
\end{equation}
where \( A_n = k_{n+1}/k_n \), \( B_n = b_nk_{n+1}/k_n \), and
\( C_n = \lambda_nk_{n+1}/k_{n-1} \).

Conversely, from the recurrence \eqref{eq:3-rr-nonmonic}, the leading
coefficient of \( P_n(x) \) is
\( k_n = A_{n-1}A_{n-2} \cdots A_0 k_0 \). Hence
\[
  \hat{p}_n(x) = (A_{n-1}A_{n-2} \cdots A_0 k_0)^{-1} P_n(x),
\]
and we can obtain the recurrence \eqref{eq:3-rr-monic} by dividing
\eqref{eq:3-rr-nonmonic} by \( A_{n}A_{n-1} \cdots A_0 k_0 \).

\begin{exam}
  Since
  \[
    \cos(n+1)\theta + \cos(n-1)\theta
    = 2 \cos\theta \cos n\theta, \qquad n\ge1,
  \]
  we have
  \[
    T_{n+1}(x) = 2 x T_n(x) - T_{n-1}(x) , \qquad n\ge1.
  \]
  Since \( T_{0}(x)=1 \) and \( T_{1}(x)=x \), we have
  \begin{equation}\label{eq:6}
    T_{n+1}(x) = A_n x T_n(x) - T_{n-1}(x), \qquad n\ge0,
  \end{equation}
  where \( T_{-1}(x) = 0 \), \( A_0=1 \) and \( A_n=2 \) for
  \( n\ge1 \). Thus the monic Tchebyshev polynomials are given by
  \( \hat{T}_n(x) = 2^{1-n} T_n(x) \) for \( n\ge1 \).
  Dividing \eqref{eq:6} by \( 2^n \) gives
  \begin{equation}\label{eq:7}
    \hat{T}_{n+1}(x) = x \hat{T}_n(x) - \lambda_n\hat{T}_{n-1}(x), \qquad n\ge0,
  \end{equation}
  where \( \lambda_1 = 1/2 \) and \( \lambda_n = 1/4 \) for \( n\ge2 \).
\end{exam}

Note that in the recurrence \eqref{eq:7} for the (monic) Tchebyshev
polynomials, \( b_n = 0 \). This, in fact, implies that
\( T_{2n}(x) \) is an even function and \( T_{2n+1}(x) \) is an odd
function. It also turns out that the odd moments are all zero.

\begin{defn}
  A linear functional \( \LL \) is \emph{symmetric} if all of its odd
  moments are zero.
\end{defn}

\begin{thm}
  Let \( \LL \) be a linear functional with monic OPS
  \( \{ P_n(x) \}_{n\ge 0} \). The following are equivalent:
  \begin{enumerate}
  \item \( \LL \) is symmetric.
  \item \( P_n(-x) = (-1)^n P_n(x) \) for \( n\ge0 \).
  \item In the 3-term recurrence \eqref{eq:3-rr}, \( b_n=0 \) for \( n\ge0 \).
  \end{enumerate}
\end{thm}

\begin{proof}
  \( (1) \Rightarrow (2) \): Since \( \LL \) is symmetric,
  \( \LL(\pi(-x)) = \LL(\pi(x)) \) for all polynomials \( \pi(x) \).
  Thus
  \( \LL(P_m(-x)P_n(-x)) = \LL(P_m(x)P_n(x)) = K_n \delta_{m,n} \).
By the uniqueness of orthogonal polynomials, \Cref{thm:uniqueness-OPS},
we have \( P_n(-x) = c_n P_n(x) \) for some \( c_n\ne 0 \).
Comparing their leading coefficients, we obtain \( c_n = (-1)^n \).

\( (2) \Rightarrow (1) \): Since \( P_{2n+1}(-x) = -P_{2n+1}(x) \),
\( P_{2n+1}(x) \) is an odd polynomial. Thus \( \LL(P_{2n+1}(x))=0 \)
is a sum of odd moments. This shows by induction that all odd moments
are zero.

\( (2) \Leftrightarrow (3) \): Let \( Q_n(x) = (-1)^n P_n(-x) \). Then
the condition in (2) is the same as \( P_n(x) = Q_n(x) \). By
\Cref{thm:3-RR}, we have
\begin{align*}
    P_{n+1}(x) &= (x-b_n) P_n(x) - \lambda_n P_{n-1}(x),\\
    Q_{n+1}(x) &= (x+b_n) Q_n(x) - \lambda_n Q_{n-1}(x),
\end{align*}
where the second recurrence is obtained from the first by replacing
\( x \) by \( -x \) and multiplying both sides by \( (-1)^{n+1} \).
Clearly, the condition \( P_n(x) = Q_n(x) \)
is equivalent to \( b_n = 0 \), \( n\ge0 \).
\end{proof}


Recall \Cref{thm:3-RR}, which states that orthogonal polynomials
satisfy a 3-term recurrence. The converse of this theorem is also
true.

\begin{thm}[Favard's theorem]
  Suppose \( \{b_n\}_{n\ge0} \) and \( \{\lambda_n\}_{n\ge1} \) are
  sequences of complex numbers such that \( \lambda_n\ne0 \) for all
  \( n\ge1 \). Let \( \{ P_n(x) \}_{n\ge 0} \) be the polynomials
  defined by \( P_{-1}(x) = 0 \), \( P_{0}(x) = 1 \), and
  \begin{equation}\label{eq:9}
    P_{n+1}(x) = (x-b_n) P_n(x) - \lambda_n P_{n-1}(x), \qquad n\ge 0.
  \end{equation}
  Then there is a (unique) linear functional \( \LL \) with
  \( \LL(1) = 1 \) for which \( \{ P_n(x) \}_{n\ge 0} \) is an OPS
  if and only if \( \lambda_n\ne 0 \) for all \( n\ge1 \).

  Moreover, \( \LL \) is positive-definite if and only if
  \( \lambda_n>0 \) for all \( n\ge1 \).
\end{thm}

\begin{proof}
  The ``only if'' part is done in \Cref{thm:3-RR}. To prove the ``if''
  part, we assume \( \lambda_n\ne 0 \) for all \( n\ge1 \). Note that
  if \( \{ P_n(x) \}_{n\ge 0} \) is an OPS for \( \LL \), then we must
  have \( \LL(P_n(x)) = 0 \) for \( n\ge1 \). This together with
  \( \LL(1) = 1 \) completely determines the moments of \( \LL \).
  Thus we define \( \LL \) to be the unique linear functional such
  that \( \LL(1) = 1 \) and \( \LL(P_n(x)) = 0 \) for \( n\ge1 \). We
  need to show that \( \{ P_n(x) \}_{n\ge 0} \) is indeed an OPS for
  \( \LL \). By \Cref{thm:orth-equiv}, it suffices to show that
  \begin{equation}\label{eq:8-2}
    \LL(x^k P_n(x)) = \lambda_1 \cdots \lambda_n \delta_{k,n}, \qquad 0\le k\le n.
  \end{equation}
  We will prove this by induction on \( k \). By the
  constriction of \( \LL \), \eqref{eq:8-2} is true when \( k=0 \).
  Let \( k\ge1 \) and suppose that \eqref{eq:8-2} holds for \( k-1 \).
  To prove \eqref{eq:8-2} for \( k \), consider an integer
  \( n\ge k \). Multiplying \( x^{k-1} \) to \eqref{eq:9}, we get
  \[
    x^k P_n(x) = x^{k-1} P_{n+1}(x) + b_n x^{k-1}P_n(x)  + \lambda_n x^{k-1} P_{n-1}(x).
  \]
  By the induction hypothesis, taking \( \LL \) in the above formula
  gives
  \[
    \LL(x^k P_n(x)) =
    \begin{cases}
     0 & \mbox{if \( 1\le k\le n-1 \)},\\
     \lambda_n \LL(x^{n-1} P_{n-1}(x)) & \mbox{if \( k=n \).}
    \end{cases}
  \]
  Thus \eqref{eq:8-2} also holds for \( k \), and the claim is
  established.

  The ``moreover'' statement follows from \Cref{cor:la-pos-def}.
\end{proof}

\subsection{Christoffel--Darboux identities
and zeros of orthogonal polynomials}

The Christoffel--Darboux identities are useful identities which have
many applications in the theory of orthogonal polynomials. In this
subsection we prove these identities and and their application to the
zeros of orthogonal polynomials.


\begin{thm}[The Christoffel--Darboux identities]
  Let \( \{ P_n(x) \}_{n\ge 0} \) be given by the 3-term recurrence
  \eqref{eq:3-rr}. For \( n\ge0 \), we have
\begin{align}
  \label{eq:CD1}
  \sum_{k=0}^{n} \frac{P_k(x)P_k(y)}{\lambda_1\cdots\lambda_k}
  &= \frac{P_{n+1}(x)P_{n}(y) - P_{n+1}(y)P_{n}(x)}{\lambda_1\cdots\lambda_{n}(x-y)}, \\
  \label{eq:CD2}
  \sum_{k=0}^{n} \frac{P_k(x)^2}{\lambda_1\cdots\lambda_k}
  &= \frac{P'_{n+1}(x)P_{n}(x) - P_{n+1}(x)P'_{n}(x)}{\lambda_1\cdots\lambda_{n}}.
\end{align}
\end{thm}
\begin{proof}
  Multiply \( P_n(y) \) to \eqref{eq:3-rr} to get
  \begin{equation}\label{eq:10}
    P_{n+1}(x) P_n(y) = (x-b_n) P_n(x) P_n(y) - \lambda_n P_{n-1}(x) P_n(y).
  \end{equation}
  Interchanging \( x \) and \( y \) in \eqref{eq:10} gives
  \begin{equation}\label{eq:11}
    P_{n+1}(y) P_n(x) = (y-b_n) P_n(x) P_n(y) - \lambda_n P_{n-1}(y) P_n(x).
  \end{equation}
  Subtracting \eqref{eq:11} from \eqref{eq:10}, we have
  \[
    P_{n+1}(x) P_n(y) - P_{n+1}(y) P_n(x) = (x-y) P_n(x) P_n(y)
    - \lambda_n (P_{n-1}(x) P_n(y) - P_{n-1}(y) P_n(x)).
  \]
  Let \( f_k = P_{k+1}(x)P_{k}(y) - P_{k+1}(y)P_{k}(x) \). Then we can
  rewrite the above equation (with \( n \) replaced by \( k \)) as
  \[
    (x-y) P_k(x)P_k(y) = f_{k} - \lambda_k f_{k-1}.
  \]
  Dividing both sides by \( \lambda_1\cdots\lambda_k(x-y) \) gives
  \[
    \frac{P_k(x)P_k(y)}{\lambda_1\cdots\lambda_k} =
    \frac{f_{k}}{\lambda_1\cdots\lambda_k(x-y)} -
    \frac{f_{k-1}}{\lambda_1\cdots\lambda_{k-1}(x-y)}.
  \]
  Summing the equation for \( k=0,\dots,n \), we obtain \eqref{eq:CD1}.

  Rewriting  \eqref{eq:CD1} as
  \[
  \sum_{k=0}^{n} \frac{P_k(x)P_k(y)}{\lambda_1\cdots\lambda_k}
    = \frac{(P_{n+1}(x)-P_{n+1}(y))P_{n}(y) -
      P_{n+1}(y)(P_{n}(x)-P_{n}(y))}{\lambda_1\cdots\lambda_{n}(x-y)}
  \]
  and taking the limit \( y\to x \) gives \eqref{eq:CD2}.
\end{proof}

The Christoffel--Darboux identities have an interesting application on
the zeros of orthogonal polynomials. We first show that orthogonal
polynomials have distinct real zeros if \( \LL \) is
positive-definite.

\begin{lem}\label{lem:real-roots}
  Let \( \LL \) be a positive-definite linear functional with monic
  OPS \( \{ P_n(x) \}_{n\ge 0} \). Then \( P_n(x) \) has \( n \)
  distinct real roots for all \( n\ge1 \).
\end{lem}

\begin{proof}
  Since \( \LL(P_n(x)) = 0 \), \( P_n(x) \) must have a root of odd
  multiplicity. (Because otherwise \( P_n(x)\ge0 \) for all
  \( x\in\RR \), which in turn implies \( \LL(P_n(x))>0 \) by the
  assumption that \( \LL \) is positive-definite.) Let
  \( x_1,\dots,x_k \) be the distinct roots of \( P_n(x) \) with odd
  multiplicities. Then \( (x-x_1) \cdots (x-x_k)P_n(x)\ge0 \) for all
  \( x\in \RR \). Therefore
  \( \LL((x-x_1) \cdots (x-x_k)P_n(x)) > 0 \). But by
  \Cref{thm:orth-equiv} this implies \( k\ge n \). Clearly,
  \( k\le n \) and we obtain \( k=n \). This means that \( P_n(x) \)
  has \( n \) distinct roots.
\end{proof}



\begin{thm}\label{thm:zero-interlace}
  Let \( \LL \) be a positive-definite linear functional with monic
  OPS \( \{ P_n(x) \}_{n\ge 0} \). Then \( P_n(x) \) has \( n \)
  distinct real roots for all \( n\ge1 \) and the zeros of
  \( P_n(x) \) and \( P_{n+1}(x) \) interlace. More precisely, if
  \( x_{n,1}>x_{n,2} > \cdots > x_{n,n} \) are the zeros of
  \( P_n(x) \), then
  \begin{equation}\label{eq:interlacing}
    x_{n+1,1} > x_{n,1} > x_{n+1,2} >x_{n,2} > \cdots >
    x_{n+1,n}> x_{n,n} >x_{n+1,n+1}.
  \end{equation}
\end{thm}
\begin{proof}
  The first part is proved in \Cref{lem:real-roots}. For the second
  part, we substitute \( x=x_{n,j} \) in \eqref{eq:CD2} to get
\[
0< \sum_{k=0}^{n} \frac{P_k(x_{n,j})^2}{\lambda_1\cdots\lambda_k} =
  \frac{P'_{n+1}(x_{n,j})P_{n}(x_{n,j}) -
    P_{n+1}(x_{n,j})P'_{n}(x_{n,j})}{\lambda_1\cdots\lambda_{n}} =
  \frac{ -
    P_{n+1}(x_{n,j})P'_{n}(x_{n,j})}{\lambda_1\cdots\lambda_{n}}.
\]
This implies that the sign of \( P_{n+1}(x_{n,j}) \) is the opposite
of the sign of \( P'_{n}(x_{n,j}) \). Considering the graph of
\( y=P_n(x) \), the sign of \( P'_n(x_{n,j}) \) is \( (-1)^{j-1} \),
see \Cref{fig:image2}. Thus the sign of \( P_{n+1}(x_{n,j}) \), for
\( j=1,2,\ldots,n \), is \( (-1)^j \) as indicated by the blue dots in
\Cref{fig:image2}.
\begin{figure}
  \centering
  \includegraphics[scale=.1]{./figures/image2.jpeg}
  \caption{The interchanging zeros of \( P_n(x) \) and \( P_{n+1}(x) \).}
  \label{fig:image2}
\end{figure}
This means that \( P_{n+1}(x) \) has a root between each interval
\( (x_{n,j+1},x_{n,j}) \) for \( j=1,\dots,n-1 \). Considering the
limits \( \lim_{x\to \infty} P_{n+1}(x) = \infty\) and
\( \lim_{x\to -\infty} P_{n+1}(x) = (-1)^{n+1} \infty\), we can see
that \( P_{n+1}(x) \) also has one root in \( (x_{n,1},\infty) \) and
one root in \( (-\infty,x_{n,n}) \). Thus we obtain
\eqref{eq:interlacing}.
\end{proof}

\section{Basics of enumerative combinatorics}

In this section we review fundamental objects in enumerative
combinatorics.
From now on we will use the notation \( [n] := \{ 1,\dots,n\} \).

\subsection{Formal power series and generating functions}

In this subsection, we study basics of formal power series and generating
functions.

A \emph{power series} is a series of the form
\[
 f(x) = a_0 + a_1 x + a_2 x^2 + \cdots.
\]
If \( a_i \)'s are real numbers, then \( f(x) \) may be considered as
a function on \( x \) whose domain is the set of real numbers \( x \)
such that the above infinite series converges.

For example, if
\[
  f(x) = 1+x+x^2 + \cdots,
\]
then we have \( f(x) = 1/(1-x) \) for \( |x|<1 \). Thus we can write,
for \( |x|<1 \),
\begin{equation}\label{eq:geometric-series}
  1+x+x^2 + \cdots = \frac{1}{1-x}.
\end{equation}
This, however, does not make sense if \( |x|>1 \). Hence, in calculus,
whenever we consider a power series we always have to mention for what
values of \( x \) the series converges. But in formal power series the
convergence is not needed.

Let \( R \) be a commutative ring with identity. Recall that
\( R[x] \) denotes the ring of polynomials in \( x \) with
coefficients in \( R \).

\begin{defn}
  The \emph{ring of formal power series} in \( x \) with coefficients
  in \( R \) is the set
\[
  R[[x]] = \{a_0 + a_1 x + a_2x^2 + \cdots : a_0,a_1,a_2,\ldots \in R \},
\]
with addition
\[
  \left( \sum_{i=0}^\infty a_i x^n \right) + \left( \sum_{i=0}^\infty
    b_i x^n \right) = \sum_{i=0}^\infty (a_i+b_i) x^n ,
\]
and multiplication
\[
  \left( \sum_{i=0}^\infty a_i x^n \right) \left( \sum_{i=0}^\infty
    b_i x^n \right) = \sum_{i=0}^\infty \left( \sum_{k=0}^n a_k
    b_{n-k} \right)x^n.
\]
\end{defn}

So, roughly speaking, a formal power series is a polynomial of
infinite degree.

The multiplicative identity of \( R[[1]] \) is \( 1 \), that is,
\( 1+0x+0x^2+\cdots \). For \( f(x),g(x) \in R[[1]] \), if
\( f(x) g(x) = 1 \), then we say that \( f(x) \) is the \emph{inverse}
of \( g(x) \) and write \( f(x) = g(x)^{-1} = 1/g(x) \).

In the language of formal power series, \eqref{eq:geometric-series} is
a perfectly valid identity without any convergence considered because
\[
  (1+x+x^2 + \cdots)(1-x) = (1+x+x^2 + \cdots) - x(1+x+x^2 + \cdots) = 1.
\]

An important aspect of formal power series is that the coefficient of
\( x^n \) must be computed using a finitely many additions and
multiplications in \( R \).


\begin{exam}
  The series
\[
  e^{1+x} = \sum_{n\ge 0} \frac{(1+x)^n}{n!}
\]
is not a formal power series in \( \RR[[x]] \) because the constant
term (the coefficient of \( x^0 \)) is \( \sum_{n\ge0} 1/n! \), which
cannot be computed by a finite number of additions and multiplications
in \( \RR \) (although we know \( \sum_{n\ge0} 1/n! = e \)). On the
other hand,
\[
  e\cdot e^{x} = \sum_{n\ge 0} \frac{e x^n}{n!}
\]
is a formal power series in \( \RR[[x]] \). 
\end{exam}

Note that being a formal power series is all about how the series is
presented rather than what values the series take as a function. Most
of the time, we will not consider a formal power series as a function.

We can naturally extend the definition of formal power series to the
multivariate case.


\begin{defn}
  Let \( \vx =(x_1,x_2,\dots) \) be a sequence of variables. Let
  \( Z \) denote the set of sequences
  \( I=(i_1,i_2,\dots)\in \ZZ_{\ge0}^\infty \) such that
  \( i_1+i_2 + \cdots < \infty \). For \( I=(i_1,i_2,\dots)\in Z \),
  we write \( \vx^I = x_1^{i_1} x_2^{i_2} \cdots \). The \emph{ring of
    formal power series} in \( x_1,x_2,\dots \) with coefficients in
  \( R \) is the set
\[
  R[[\vx]] = \left\{ \sum_{I\in Z} a_I \vx^I  : a_I\in R \right\},
\]
with addition
\[
  \left(\sum_{I\in Z} a_I \vx^I \right) + \left(\sum_{I\in Z} b_I \vx^I
  \right) = \left( \sum_{I\in Z} (a_I+b_I) \vx^I \right),
\]
and multiplication
\[
  \left(\sum_{I\in Z} a_I \vx^I \right) \left(\sum_{I\in Z} b_I \vx^I
  \right) = \sum_{I\in Z} \left( \sum_{I_1,I_2\in Z, I_1+I_2=I} a_{I_1} b_{I_2} \right) \vx^I.
\]
\end{defn}

Again, rougly speaking, a multivariate formal power series is a
multivariate polynomial of infinite degree.


Now we define the notion of generating functions.


\begin{defn}
  The \emph{generating function} for a sequences \( \{ a_n\}_{n\ge 0} \) is
  defined to be the formal power series
\[
  a_0 + a_1 x + a_2 x^2 + \cdots.
\]
\end{defn}

So, the generating function for \( \{ a_n\}_{n\ge 0} \) is nothing but
a way of recording the sequence. One of the benefits of generating
functions is that we can use many properties of formal power series.


We can easily extend the definition of generating functions to
accommodate arrays \( \{ a_I\}_{I\in Z} \) of elements \( a_I\in R \)
using multivariate formal power series. More generally, we will
consider generating functions for arbitrary (combinatorial) objects.

\begin{defn}
  Let \( A \) be a set of objects. A \emph{weight} on \( A \) is a
  function \( \wt:A\to R[[\vx]] \). The \emph{generating function} for
  \( A \) with respect to the weight function \( \wt \) is the formal
  power series
  \[
    \sum_{a \in A} \wt(a),
  \]
  provided that this is a valid formal power series in \( R[[\vx]] \).
\end{defn}


\begin{exam}
  Let \( A = \{0,1,2,\dots\} \) and define a weight of \( A \) by
  \( \wt(a) = x^a \). Then the generating function for \( A \)
  (with this weight) is
  \[
    \sum_{a \in A} \wt(a) = \sum_{n=0}^n \wt(n)
    = \sum_{n=0}^n x^n = \frac{1}{1-x}.
  \]
\end{exam}

\begin{exam}
  Let \( A \) be the set of subsets of \( [n] \) and define a weight
  of \( A \) by \( \wt(a) = x^{|a|} \). Then the generating function for
  \( A \) (with this weight) is
  \[
    \sum_{a \in A} \wt(a) = \sum_{a\subseteq [n]} x^{|a|} =
    \sum_{k=0}^n \binom{n}{k} x^{k} = (1+x)^n.
  \]
\end{exam}

\begin{exam}
  Let \( A \) be the set \( S_n \) of permutations of \( [n] \) and
  define a weight of \( A \) by \( \wt(a) = x^{\cycle(a)} \). Then it
  can be proved that the generating function for \( A \) (with this
  weight) is
  \[
    \sum_{a \in A} \wt(a) = \sum_{\pi\in S_n} x^{\cycle(a)} = x(x+1)
    \cdots (x+n-1).
  \]
\end{exam}

We will often use the term ``generating function'' in a flexible
manner. For example, the generating function for the number of
permutations would mean the generating function for the sequence
\( \{a_n = n!\}_{n\ge0} \), that is, \( \sum_{n\ge0} n! x^n \).

\subsection{Dyck paths and Motzkin paths}

\subsection{Matchings, set partitions, permutations}

\section{Combinatorial interpretations for orthogonal polynomials and
  their moments}
\label{sec:comb-interpr-orth}

From now one we will focus on the combinatorial approaches to
orthogonal polynomials in Viennot's lecture notes \cite{ViennotLN}.
Part of this section will have some overlaps with \Cref{sec:basics-orth-polyn}.

The main goal of this section to give combinatorial interpretations
for orthogonal polynomials and their moments. Using these
combinatorial interpretations, we will reprove the orthogonality of
orthogonal polynomials using combinatorics only.

\subsection{Orthogonal polynomials and 3-term recurrences}

In this subsection we give basic definitions and prove simple but
useful lemmas. We then state the 3-term recurrence of orthogonal
polynomials and Favard's theorem.

Let \( K \) be a field. We denote by \( K[x] \) the ring of
polynomials in \( x \) with coefficients in \( K \). A \emph{linear
  functional} is a linear transformation \( \LL:K[x]\to K \), i.e., a
function satisfying \( \LL(af(x)+bg(x)) = a \LL(f(x))+b \LL(g(x)) \)
for all \( f(x),g(x)\in K[x] \) and \( a,b\in K \). The \( n \)th
\emph{moment} of \( \LL \) is defined to be \( \mu_n = \LL(x^n) \).

\begin{defn}\label{def:formal-ops}
  Let \( \LL \) be a linear functional defined on the space of
  polynomials in \( x \). A sequence of polynomials
  \( \{P_n(x)\}_{n\ge0} \) is called an \emph{orthogonal polynomial
    sequence (OPS)} with respect to \( \LL \) if the following
  conditions hold:
  \begin{enumerate}
  \item \( \deg P_n(x) = n \), \( n\ge0 \),
  \item \( \LL(P_m(x)P_n(x))  = 0 \) for \( m\ne n \),
  \item \( \LL(P_m(x)^2) \ne 0 \) for \( m\ge0 \).
  \end{enumerate}
  We also say that \( \{ P_n(x) \}_{n\ge 0} \) is orthogonal
  for the moments \( \{\mu_n\}_{n\ge0} \).
\end{defn}

Orthogonal polynomials in the above definition are called ``formal''
or ``general'' orthogonal polynomials because the field \( K \) can be
anything. For instance, it may contain arbitrary formal variables such
as \( a,b,c,d \). Then the polynomials \( P_n(x) \) and the moments
\( \mu_n \) can be treated as polynomials (or more complicated objects
such as formal power series or rational functions) in these formal
variables.

\begin{prop}
  Suppose that \( \{ P_n(x) \}_{n\ge 0} \) is an OPS for \( \LL \).
\begin{enumerate}
\item \( \{ P_n(x) \}_{n\ge 0} \) is also orthogonal with respect to
  \( \LL' \) for any \( \LL' = a\LL \) for \( a\ne 0 \).
\item \( \LL \) is uniquely determined up to
  nonzero scalar multiplication. 
\item If we set \( \LL(1) = 1 \), then \( \LL \) is uniquely
  determined.
\item \( \{ a_nP_n(x) \}_{n\ge 0} \) is an OPS with respect to
  \( \LL \) for any sequence \( \{ a_n\}_{n\ge 0} \) with
  \( a_n\ne 0 \).
\end{enumerate}
\end{prop}

\begin{proof}
  All statements are easy to check. For example, (2) can be seen by
  noticing that once the \( 0 \)th moment \( \mu_0=\LL(1) \) is
  determined, then the \( n \)th moment \( \mu_n \), for \( n\ge1 \),
  is uniquely determined by the condition \( \LL(P_n(x)) = 0 \).
\end{proof}

By the above proposition we may assume that \( \LL(1) = 1 \).
\emph{From now on we will always assume that \( \deg P_n(x) = n \) and
  \( \LL(1) = 1 \) unless otherwise stated.}

Recall from \Cref{thm:3-RR} that every OPS satisfies a 3-term
recurrence.

\begin{thm}[3-term recurrence]
  Let \( \LL \) be a linear functional with monic OPS
  \( \{ P_n(x) \}_{n\ge 0} \). Then there are sequences
  \( \{ b_n\}_{n\ge 0} \) and \( \{ \lambda_n\}_{n\ge 1} \) such that
  \( \lambda_n\ne 0 \) and
  \[
    P_{n+1}(x) = (x-b_n) P_n(x) - \lambda_n P_{n-1}(x), \qquad n\ge0,
  \]
  where \( P_{-1}(x) = 0 \) and \( P_0(x) = 1 \).
\end{thm}

The inverse of the above theorem is also true, which is one of the
most important results in the theory of classical orthogonal
polynomials.

\begin{thm}[Favard's theorem]
  Let \( \{ P_n(x) \}_{n\ge 0} \) be a sequence of monic polynomials.
  Then \( \{ P_n(x) \}_{n\ge 0} \) is an OPS for some linear
  functional \( \LL \) if and only if
  there are sequences \( \{ b_n\}_{n\ge 0} \) and \( \{ \lambda_n\}_{n\ge 1} \)
  such that \( \lambda_n\ne 0 \) and
  \begin{equation}\label{eq:3-RR}
    P_{n+1}(x) = (x-b_n) P_n(x) - \lambda_n P_{n-1}(x), \qquad n\ge0,
  \end{equation}
  where \( P_{-1}(x) = 0 \) and \( P_0(x) = 1 \).
\end{thm}

The main goal of this section is to give combinatorial interpretations
for the orthogonal polynomials \( P_n(x) \) and their moments
\( \mu_n \). Using these combinatorial interpretations we will prove
Favard's theorem bijectively.

\subsection{A model for orthogonal polynomials using Favard tilings}

In this subsection we give a combinatorial interpretation for
orthogonal polynomials using Favard tilings.


\begin{defn}
  A \emph{Favard tiling of size $n$} is a tiling of a $1\times n$
  square board with monominos and dominos, where each monomino may be
  marked. The set of Favard tilings of size $n$ is denoted by $\FT_n$.

  We label the squares in the $1\times n$ board by $0,1,\dots,n-1$
  from left to right. Define the weight \( \wt(T) \) of
  \( T\in \FT_n \) to be the product of the weights of the tiles in
  \( T \), where
  \begin{enumerate}
  \item the weight of each marked monomino is \( x \),
  \item the weight of each unmarked monomino containing a label
    \( i \) is \( -b_i \), and
  \item the weight of each domino containing labels \( i-1 \) and
    \( i \) is \( -\lambda_i \).
  \end{enumerate}




\end{defn}

For example, see \Cref{fig:tiling}. Note that the number \( u_n \) of
Favard tilings of size \( n \) satisfies \( u_{n+1} = 2u_n+u_{n-1} \)
with \( u_0=1 \) and \( u_1=2 \). These numbers are called the Pell
numbers.

\begin{figure}
  \centering
\begin{tikzpicture}[scale=0.75]
  \draw [help lines] (0,0) grid (8,1);
  \foreach \x in {0,...,7} \draw node at (\x+0.5,0.5) {\x};
  \LBD1 \LRM2 \LBM3 \LBM4 \LRM5 \LRM6 \LBM7
\end{tikzpicture}
\caption{A Favard tiling $T\in\FT_8$ with
  $\wt(T)=\lambda_1 b_3b_4b_7x^3$. The marked monominos are colored
  red.}
  \label{fig:tiling}
\end{figure}

The following proposition gives a combinatorial interpretation for
orthogonal polynomials.

\begin{prop}\label{pro:Favard}
  Suppose that \( \{ P_n(x) \}_{n\ge 0} \) is a sequence of
  polynomials satisfying \eqref{eq:3-RR}. Then
  \[
    P_n(x) = \sum_{T\in \FT_n} \wt(T).
  \]
\end{prop}
\begin{proof}
  This is immediate from the recurrence \eqref{eq:3-RR}.
\end{proof}




\begin{figure}
  \centering
\begin{tikzpicture}[scale=0.75]
    \draw[help lines] (0,0) grid (13,3);
    \foreach \y in {0,...,3} \draw node at (-.5,\y) {$\y$};
    \draw[line width = 1.5pt] (0,3)-- ++(0,-1)-- ++(0,-1)-- ++(1,-1)-- ++(1,1)-- ++(1,1)-- ++(0,-1)-- ++(1,-1)-- ++(1,0)-- ++(1,1)-- ++(1,1)-- ++(1,1)-- ++(1,-1)-- ++(1,0)-- ++(0,-1)-- ++(1,1)-- ++(1,0)-- ++(0,-1)-- ++(1,0);
    \node at (0.4,2.5) {$a_{3}$};
    \node at (0.4,1.5) {$a_{2}$};
    \node at (0.8,0.8) {$\lambda_{1}$};
    \node at (3.4,1.5) {$a_{2}$};
    \node at (3.8,0.8) {$\lambda_{1}$};
    \node at (4.5,0.3) {$b_{0}$};
    \node at (8.8,2.8) {$\lambda_{3}$};
    \node at (9.5,2.3) {$b_{2}$};
    \node at (10.4,1.8) {$a_{2}$};
    \node at (11.5,2.3) {$b_{2}$};
    \node at (12.4,1.8) {$a_{2}$};
    \node at (12.7,1.3) {$b_{1}$};
\end{tikzpicture}
\caption{A Motzkin-Schr\"oder path $\pi$ from $(0,3)$ to $(13,1)$ with
  $\wt(\pi)=a_2^4a_3b_0b_1b_2^2\lambda_1^2\lambda_3$.}
  \label{fig:MS}
\end{figure}



\subsection{How to find a combinatorial model for moments}

Moments are important quantities of a linear functional \( \LL \)
because they have all the information of \( \LL \). In this subsection
we will find a combinatorial interpretation for the moments of
orthogonal polynomials. To do this we will first take a close look at
the moments.

Suppose that \( \LL \) is a linear functional with monic OPS
\( \{ P_n(x) \}_{n\ge 0} \), which satisfies the 3-term recurrence
\eqref{eq:3-RR}. Let's assume \( \LL(1)=1 \). Then, using the
orthogonality, we have
\begin{equation}\label{eq:13}
  \LL(P_n(x)) = \delta_{n,0}.
\end{equation}
This relation in fact completely determines the moments \( \mu_n \).
For example, since
\begin{align*}
  P_0(x) &= 1,\\
  P_1(x) &= (x-b_0)P_0(x) - \lambda_0P_{-1}(x) = x-b_0,\\
  P_2(x) &= (x-b_1)P_1(x) - \lambda_1P_0(x) =  x^2 - (b_1+b_0)x +b_0b_1 - \lambda_1,
\end{align*}
we have
\begin{align*}
 \mu_0 &= \LL(1) = 1, \\
 \mu_1 &= \LL(x) = \LL(P_1(x)+b_0) = b_0, \\
  \mu_2 &= \LL(x^2) = \LL(P_2(x)+(b_0+b_1)x-b_0b_1+\lambda_1)
          = (b_0+b_1)b_0 - b_0b_1 +\lambda_1 = b_0^2+\lambda_1.
\end{align*}
In this way, we can compute a few more moments:
\begin{align*}
  \mu_3 &= b_0^3 + 2b_0\lambda_1 + b_1\lambda_1,\\
  \mu_4 &= b_0^4 + 3b_0^2\lambda_1 + 2b_0b_1\lambda_1 + b_1^2\lambda_1 + \lambda_1^2 + \lambda_1\lambda_2,\\
  \mu_5 &= b_0^5 + 4b_0^3\lambda_1 + 3b_0^2b_1\lambda_1 + 2b_0b_1^2\lambda_1 + b_1^3\lambda_1 + 3b_0\lambda_1^2 + 2b_1\lambda_1^2 + 2b_0\lambda_1\lambda_2 + 2b_1\lambda_1\lambda_2 + b_2\lambda_1\lambda_2.
\end{align*}

The above experiments clearly suggest that \( \mu_n \) would be a
polynomial in \( b_i \)'s and \( \lambda_i \)'s with nonnegative
integer coefficients. How can we prove such a conjecture? A satisfying
answer to this question is to find combinatorial objects whose
generating function is \( \mu_n \). That is to find a set \( X \) of
combinatorial objects and a weight \( \wt(A) \) of each element
\( A\in X \) such that
\[
  \mu_n = \sum_{A \in X} \wt(A),
\]
and \( \wt(A) \) is a polynomial (preferably a monomial) in
\( b_i \)'s and \( \lambda_i \)'s with nonnegative integer
coefficients.

But how can we find such combinatorial objects? Suppose that such
combinatorial objects exist with monomial weight \( \wt(A) \) for each
\( A\in X \). Then if we set \( b_i=\lambda_i=1 \) for all \( i \)
then \( \mu_n \) would be the number of elements in \( X \). If we
compute \( \mu_n \) with this substitution for \( n=0,1,2,\ldots \),
then we obtain the following sequence:
\[
  1, 1, 2, 4, 9, 21, 51, 127, 323, 835, 2188, 5798, 15511, \ldots.
\]
There is a very useful webpage \url{https://oeis.org/} where you can
search integer sequences. If you search the above sequence, the
webpage will tell you that this is the sequence of the number of
Motzkin paths. So we can guess that there must be a close connection
with the moments of orthogonal polynomials and Motzkin paths. See
\Cref{sec:dyck-paths-motzkin} for more information on Motzkin paths.


Let \( \Motz_n \) denote the set of Motzkin paths from \( (0,0) \) to
\( (n,0) \). Define the weight \( \wt(\pi) \) of a Motzkin path \( \pi\in \Motz_n \)
to be the product of the weights of the steps in \( \pi \),
\begin{enumerate}
\item the weight of an up step is \( 1 \),
\item the weight of a horizontal step starting at height \( i \) is \( b_i \), and
\item the weight of a down step starting at height \( i \) is
  \( \lambda_i \).
\end{enumerate}

After spending enough time of trials and errors, we can come up with
the following combinatorial model for the moments of orthogonal
polynomials.

\begin{thm}\label{thm:mu=Mot}
  Suppose that \( \{ P_n(x) \}_{n\ge 0} \) is a monic OPS for a
  linear functional \( \LL \) with \( \LL(1)=1 \). Suppose that
  \( \{ P_n(x) \}_{n\ge 0} \) satisfy the 3-term recurrence
  \[
      P_{n+1}(x) = (x-b_n) P_n(x) - \lambda_n P_{n-1}(x), \qquad n\ge0.
  \]
  Then the moments \( \mu_n=\LL(x^n) \) are given by
\[
  \mu_n = \sum_{\pi\in \Motz_n} \wt(\pi).
\]
\end{thm}

More generally, we will prove
a combinatorial interpretation for mixed moments.

\begin{defn}
  Let \( \{ P_n(x) \}_{n\ge 0} \) be a monic OPS for a linear
  functional \( \LL \). For integers \( n,r,s\ge0 \), the \emph{mixed
    moments} \( \mu_{n,r,s} \) and \( \mu_{n,k} \) of this OPS are
  defined by
  \begin{align*}
    \mu_{n,r,s} &= \frac{\LL(x^n P_r(x)P_s(x))}{\LL(P_s(x)^2)},\\
    \mu_{n,k} &= \mu_{n,0,k} = \frac{\LL(x^n P_k(x))}{\LL(P_k(x)^2)}.
  \end{align*}
\end{defn}

Note that \( \mu_n = \mu_{n,0,0} \).

Let \( \Motz_{n,r,s} \) denote the set of Motzkin paths from
\( (0,r) \) to \( (n,s) \). The weight of \( \pi\in \Motz_{n,r,s} \)
is defined in the same manner as the weight of a Motzkin path from
\( (0,0) \) to \( (n,0) \).

\begin{thm}\label{thm:mu=Mot_nrs}
  Following the notation in \Cref{thm:mu=Mot}, we have
\[
  \mu_{n,r,s} = \sum_{\pi\in \Motz_{n,r,s}} \wt(\pi).
\]
\end{thm}

\begin{proof}
  We proceed by induction on \( n \).
  Suppose \( n=0 \). By the orthogonality of \( \{ P_n(x) \}_{n\ge 0} \),
  we have
  \[
    \mu_{0,r,s} = \frac{\LL(P_r(x)P_s(x))}{\LL(P_s(x)^2)} = \delta_{r,s}.
  \]
  Since \( \Motz_{0,r,s} = \emptyset \) if \( r=s \) and
  \( \Motz_{0,r,s} \) has only one (empty) Motzkin path if \( r=s \),
  we also have
  \( \sum_{\pi\in \Motz_{n,r,s}} \wt(\pi) = \delta_{r,s} \).

  Let \( n\ge1 \) and suppose that the theorem holds for \( n-1 \).
  Then by the 3-term recurrence,
  \[
    xP_r(x) = P_{r+1}(x) + b_rP_r(x) + \lambda_rP_{r-1}(x).
  \]
  Thus
  \begin{align*}
    \mu_{n,r,s}
    &= \frac{\LL(x^n P_r(x) P_s(x))}{\LL(P_s(x)^2)}
      = \frac{\LL(x^{n-1} (xP_r(x)) P_s(x))}{\LL(P_s(x)^2)}\\
    &=\frac{\LL(x^{n-1} (P_{r+1}(x) + b_rP_r(x) + \lambda_rP_{r-1}(x)) P_s(x))}{\LL(P_s(x)^2)} \\
    &= \frac{\LL(x^{n-1} P_{r+1}(x)P_s(x))}{\LL(P_s(x)^2)}
      + b_r\frac{\LL(x^{n-1} P_rP_s(x))}{\LL(P_s(x)^2)}
      + \lambda_r \frac{\LL(x^{n-1} P_{r-1}(x)P_s(x))}{\LL(P_s(x)^2)}\\
    &= \mu_{n-1,r+1,s} + b_r\mu_{n-1,r,s} + \lambda_r \mu_{n-1,r-1,s}\\
    &= \sum_{\pi\in \Motz_{n-1,r+1,s}} \wt(\pi)
    + b_r \sum_{\pi\in \Motz_{n-1,r,s}} \wt(\pi)
    + \lambda_r \sum_{\pi\in \Motz_{n-1,r-1,s}} \wt(\pi),
  \end{align*}
  where the last equation follows from the induction hypothesis.

  On the other hand, considering the first step of each \( \pi\in \Motz_{n,r,s} \),
  it is easy to check that 
  \[
     \sum_{\pi\in \Motz_{n,r,s}} \wt(\pi)
    = \sum_{\pi\in \Motz_{n-1,r+1,s}} \wt(\pi)
    + b_r \sum_{\pi\in \Motz_{n-1,r,s}} \wt(\pi)
    + \lambda_r \sum_{\pi\in \Motz_{n-1,r-1,s}} \wt(\pi) .
  \]
  Hence the theorem holds for \( n \) and we are done by induction.
\end{proof}


\subsection{A bijective proof of Favard's theorem}

We have a combinatorial interpretation for both orthogonal polynomials
and their moments. In this section we will prove Favard's theorem
bijectively using these combinatorial models.


Suppose that \( \{ P_n(x) \}_{n\ge 0} \) is a sequence of polynomials
satisfying the 3-term recurrence
\[
  P_{n+1}(x) = (x-b_n) P_n(x) - \lambda_n P_{n-1}(x).
\]
To prove Favard's theorem, we need to find a linear functional
\( \LL \) for which \( \{ P_n(x) \}_{n\ge 0} \) are orthogonal.
We simply define \( \LL \) so that the moments are given by
\begin{equation}\label{eq:8}
  \LL(x^n) = \sum_{\pi\in \Motz_{n}} \wt(\pi).
\end{equation}
It is enough to show that
\[
  \LL(P_r(x)P_s(x)) = \lambda_1 \cdots \lambda_s \delta_{r,s}.
\]
More generally, we will prove
\begin{equation}\label{eq:LL(xPP)}
  \LL(x^n P_r(x)P_s(x)) = \lambda_1 \cdots \lambda_s \sum_{\pi\in
    \Motz_{n,r,s}} \wt(\pi).
\end{equation}

We first need to give a combinatorial meaning to the the left-hand
side of \eqref{eq:LL(xPP)}. For a Favard tiling \( T \) with \( k \)
marked monominos, let \( \wt'(T) = \wt(T)/x^k \). By \Cref{pro:Favard}
and the definition \eqref{eq:8} of \( \LL(x^n) \), we have
\[
    \LL(x^n P_r(x)P_s(x)) = 
    \sum_{(T_1,T_2,\pi)\in X} \wt'(T_1) \wt'(T_2) \wt(\pi),
\]
where \( X \) is the set of triples \( (T_1,T_2,\pi) \)
such that
\begin{enumerate}
\item \( T_1\in \FT_r \) has \( i \) marked monominos for some \( 0\le i\le r \),
\item \( T_2\in \FT_s \) has \( j \) marked monominos for some \( 0\le j\le s \), and
\item \( \pi\in \Motz_{n+i+j} \).
\end{enumerate}

Let \( Y \) be the set of \( \pi\in \Motz_{n+r+s} \) such that the
first \( r \) steps are up steps and the last \( s \) step s are down
steps. Then the right-hand side of \eqref{eq:LL(xPP)} is equal to
\( \sum_{\pi\in Y} \wt(\pi) \).

By the above observation, \eqref{eq:LL(xPP)} is equivalent to the
following theorem.

\begin{thm}
  For the sets \( X \) and \( Y \) defined above, we have
\begin{equation}\label{eq:LL(xPP)2}
  \sum_{(T_1,T_2,\pi)\in X} \wt'(T_1) \wt'(T_2) \wt(\pi)
  =  \sum_{\pi\in Y} \wt(\pi).
\end{equation}
\end{thm}

\begin{proof}
 We will find a sign-reversing involution on \( X \).

 Consider \( (T_1,T_2,\pi)\in X \). We write
 \( \pi=S_1S_2 \cdots S_{m} \) as a sequence of steps and suppose
 \begin{itemize}
 \item \( a \) is the largest integer such that \( T_1 \)
   starts with \( a \) marked monominos,
 \item \( b \) is the largest integer such that \( T_2 \)
   starts with \( a \) marked monominos,
 \item \( u \) is the largest integer such that \( \pi \) starts with
   \( u \) up steps,
 \item \( v \) is the largest integer such that \( \pi \) ends with
   \( v \) down steps.
 \end{itemize}
 We define \( \phi(T_1,T_2,\pi)= (T'_1,T'_2,\pi') \) as follows.
\begin{description}
\item[Case 1] \( u<a \). In this case we set \( T'_2 = T_2 \). There are two
  subcases.
\begin{description}
\item[Case 1-1] \( S_{u+1} \) is a horizontal step. Let
    \[
      \pi' = S_1\cdots \widehat{S_{u+1}} \cdots S_{m},
\]
and define $T_1'$ to be the Favard tiling obtained from $T_1$ by
replacing the \( (u+1) \)st marked monomino (at position \( u \)) by a
unmarked monomino. Here the notation $\widehat{S_{u+1}}$ means that
$S_{u+1}$ is removed from the sequence. See Figure~\ref{fig:case-a}.
\item[Case 1-2] $S_{u+1}$ is a down step. Let
    \[
      \pi' = S_1\cdots \widehat{S_{u}}\widehat{S_{u+1}} \cdots S_m,
\]
and define $T_1'$ to be the Favard tiling obtained from $T_1$ by
replacing the \( u \)th and \( (u+1) \)st marked monominos (at
positions $u-1$ and $u$) by a domino. See Figure~\ref{fig:case-c}.
\end{description}

\item[Case 2] \( u\ge a\ne r \). In this case we set \( T'_2=T_2 \). Let
  \( A \) be the \( (a+1) \)st tile in \( T_1 \) (\( A \) starts at position
  \( a \)). There are two subcases.
  \begin{description}
  \item[Case 2-1] $A$ is a unmarked monomino. In this case let
    \[
      \pi' = S_1\cdots S_u H S_{u+1} \cdots S_m,
\]
and define $T_1'$ to be the Favard tiling obtained from $T_1$ by
replacing $A$ by a marked monomino. See Figure~\ref{fig:case-a}.
\item[Case 2-2] $A$ is a domino. In this case let
    \[
      \pi' = S_1\dots S_u UD S_{u+1} \dots S_m,
\]
and define $T_1'$ to be the Favard tiling obtained from $T_1$ by replacing $A$ by two
marked monominos. See Figure~\ref{fig:case-c}.
\end{description}
\item[Case 3] \( u\ge a = r \) and \( v<b \). This can be done
  similarly as Case 1. The only difference is that we set
  \( T_1' = T_1 \) and consider the steps of \( \pi \) from the right.
  
\item[Case 4] \( u\ge a = r \) and \( v\ge b \ne s \).
  This can be done similarly as Case 2.

\item[Case 5] \( u\ge a = r \) and \( v\ge b=s \). In this case we set
  \( (T'_1,T'_2,\pi') = (T_1,T_2,\pi) \).
\end{description}

\begin{figure}
  \centering
\begin{tikzpicture}[scale=0.75]
    \draw[help lines] (0,0) grid (6,3);
   \draw[line width = 1.5pt] (0,0)--++(3,3)--++(1,0)--++(1,-1)--++(0,-1)--++(1,1);
   \draw[line width = 1.5pt, red] (3,3)--++(1,0);
  \draw [decorate,decoration={brace,amplitude=10pt},xshift=-0pt,yshift=-3pt]
(3,0) -- (0,0) node [black,midway,yshift=-0.6cm] {$i=3$};
\begin{scope}[shift={(0,-2.5)}]
  \draw [help lines] (0,0) grid (6,1);
  \foreach \x in {1,...,6} \draw node at (\x-0.5,0.5) {\x};
  \BM0 \BM1 \BM2 \BM3 \RD5
  \draw [decorate,decoration={brace,amplitude=10pt},xshift=-0pt,yshift=-3pt]
(4,0) -- (0,0) node [black,midway,yshift=-0.6cm] {$j=4$};
\end{scope}
\end{tikzpicture}   \qquad\qquad\qquad
\begin{tikzpicture}[scale=0.75]
    \draw[help lines] (0,0) grid (5,3);
   \draw[line width = 1.5pt] (0,0)--++(3,3)--++(1,-1)--++(0,-1)--++(1,1);
   \remove(3,3)
  \draw [decorate,decoration={brace,amplitude=10pt},xshift=-0pt,yshift=-3pt]
(3,0) -- (0,0) node [black,midway,yshift=-0.6cm] {$i=3$};
\begin{scope}[shift={(0,-2.5)}]
  \draw [help lines] (0,0) grid (6,1);
  \foreach \x in {1,...,6} \draw node at (\x-0.5,0.5) {\x};
  \BM0 \BM1 \BM2 \RM3 \RD5
  \draw [decorate,decoration={brace,amplitude=10pt},xshift=-0pt,yshift=-3pt]
(3,0) -- (0,0) node [black,midway,yshift=-0.6cm] {$j=3$};
\end{scope}
\end{tikzpicture}   
\caption{A pair $(\pi,T)\in X$ in Case 1-a on the left and the corresponding
  pair $(\pi',T')$ in Case 2-a on the right, for $(n,m,\ell)=(2,6,2)$.
The horizontal step starting at $(3,3)$ in $\pi$ is collapsed to a point.}
  \label{fig:case-a}
\end{figure}

\begin{figure}
  \centering
\begin{tikzpicture}[scale=0.75]
    \draw[help lines] (0,0) grid (6,3);
   \draw[line width = 1.5pt] (0,0)--++(3,3)--++(0,-2)--++(1,1)--++(1,0)--++(1,1)--++(0,-1);
   \draw[line width = 1.5pt, red] (2,2)--++(1,1)--++(0,-1);
  \draw [decorate,decoration={brace,amplitude=10pt},xshift=-0pt,yshift=-3pt]
(3,0) -- (0,0) node [black,midway,yshift=-0.6cm] {$i=3$};
\begin{scope}[shift={(0,-2.5)}]
  \draw [help lines] (0,0) grid (6,1);
  \foreach \x in {1,...,6} \draw node at (\x-0.5,0.5) {\x};
  \BM0 \BM1 \BM2 \BM3 \RD5
  \draw [decorate,decoration={brace,amplitude=10pt},xshift=-0pt,yshift=-3pt]
(4,0) -- (0,0) node [black,midway,yshift=-0.6cm] {$j=4$};
\end{scope}
\end{tikzpicture}   \qquad\qquad\qquad
\begin{tikzpicture}[scale=0.75]
    \draw[help lines] (0,0) grid (5,3);
   \draw[line width = 1.5pt] (0,0)--++(2,2)--++(0,-1)--++(1,1)--++(1,0)--++(1,1)--++(0,-1);
   \remove(2,2)
  \draw [decorate,decoration={brace,amplitude=10pt},xshift=-0pt,yshift=-3pt]
(2,0) -- (0,0) node [black,midway,yshift=-0.6cm] {$i=2$};
\begin{scope}[shift={(0,-2.5)}]
  \draw [help lines] (0,0) grid (6,1);
  \foreach \x in {1,...,6} \draw node at (\x-0.5,0.5) {\x};
  \BM0 \BM1 \BD3 \RD5
  \draw [decorate,decoration={brace,amplitude=10pt},xshift=-0pt,yshift=-3pt]
(2,0) -- (0,0) node [black,midway,yshift=-0.6cm] {$j=2$};
\end{scope}
\end{tikzpicture}   
\caption{A pair $(\pi,T)\in X$ in Case 1-b on the left and the corresponding
  pair $(\pi',T')$ in Case 2-b on the right, for $(n,m,\ell)=(2,6,2)$.
The peak $(U,V)$ starting at $(2,2)$ in $\pi$ is collapsed to a point.}
  \label{fig:case-b}
\end{figure}

\begin{figure}
  \centering
\begin{tikzpicture}[scale=0.75]
    \draw[help lines] (0,0) grid (6,3);
   \draw[line width = 1.5pt] (0,0)--++(3,3)--++(1,-1)--++(1,0)--++(0,-1)--++(1,1);
   \draw[line width = 1.5pt, red] (2,2)--++(1,1)--++(1,-1);
  \draw [decorate,decoration={brace,amplitude=10pt},xshift=-0pt,yshift=-3pt]
(3,0) -- (0,0) node [black,midway,yshift=-0.6cm] {$i=3$};
\begin{scope}[shift={(0,-2.5)}]
  \draw [help lines] (0,0) grid (6,1);
  \foreach \x in {1,...,6} \draw node at (\x-0.5,0.5) {\x};
  \BM0 \BM1 \BM2 \BM3 \RD5
  \draw [decorate,decoration={brace,amplitude=10pt},xshift=-0pt,yshift=-3pt]
(4,0) -- (0,0) node [black,midway,yshift=-0.6cm] {$j=4$};
\end{scope}
\end{tikzpicture}   \qquad\qquad\qquad
\begin{tikzpicture}[scale=0.75]
    \draw[help lines] (0,0) grid (4,3);
   \draw[line width = 1.5pt] (0,0)--++(2,2)--++(1,0)--++(0,-1)--++(1,1);
   \remove(2,2)
  \draw [decorate,decoration={brace,amplitude=10pt},xshift=-0pt,yshift=-3pt]
(2,0) -- (0,0) node [black,midway,yshift=-0.6cm] {$i=2$};
\begin{scope}[shift={(0,-2.5)}]
  \draw [help lines] (0,0) grid (6,1);
  \foreach \x in {1,...,6} \draw node at (\x-0.5,0.5) {\x};
  \BM0 \BM1 \RD3 \RD5
  \draw [decorate,decoration={brace,amplitude=10pt},xshift=-0pt,yshift=-3pt]
(2,0) -- (0,0) node [black,midway,yshift=-0.6cm] {$j=2$};
\end{scope}
\end{tikzpicture}   
\caption{A pair $(\pi,T)\in X$ in Case 1-c on the left and the corresponding
  pair $(\pi',T')$ in Case 2-c on the right, for $(n,m,\ell)=(2,6,2)$.
The peak $(U,D)$ starting at $(2,2)$ in $\pi$ is collapsed to a point.}
  \label{fig:case-c}
\end{figure}

\begin{figure}
  \centering
  \begin{tikzpicture}[scale=0.75]
    \draw[help lines] (0,0) grid (8,6);
   \draw[line width = 1.5pt] (0,0)--++(6,6)--++(1,0)--++(0,-3)--++(1,1)--++(0,-2);
  \draw [decorate,decoration={brace,amplitude=10pt},xshift=-0pt,yshift=-3pt]
(6,0) -- (0,0) node [black,midway,yshift=-0.6cm] {$i=6$};
\begin{scope}[shift={(0,-2.5)}]
  \draw [help lines] (0,0) grid (6,1);
  \foreach \x in {1,...,6} \draw node at (\x-0.5,0.5) {\x};
  \BM0 \BM1 \BM2 \BM3 \BM4 \BM5
  \draw [decorate,decoration={brace,amplitude=10pt},xshift=-0pt,yshift=-3pt]
(6,0) -- (0,0) node [black,midway,yshift=-0.6cm] {$j=6$};
\end{scope}
\end{tikzpicture}   
\caption{A pair $(\pi,T)\in X$ in Case 3 for $(n,m,\ell)=(2,6,2)$. In this case
  $(\pi,T) = (\pi',T')$ is a fixed point.}
  \label{fig:case3}
\end{figure}

It is not hard to check that
the map \( \phi(T_1,T_2,\pi)= (T'_1,T'_2,\pi') \)
is a sign-reversing involution on \( X \)
with fixed points \( (\emptyset,\emptyset,\pi) \)
where \( \pi\in Y \).
\end{proof}

\section{Moments of classical orthogonal polynomials}

In this section we consider
Tchebyshev polynomials of the 1st and 2nd kinds,
Laguerre polynomials. Hermite polynomials,
Charlier polynomials, and Meixner polynomials of the 1st and 2nd kinds.

Note that an OPS \( \{ P_n(x) \}_{n\ge 0} \) can be defined in many
ways, namely, one of the following determines the orthogonal
polynomials:
\begin{enumerate}
\item the coefficients \( a_{n,k} \) of \( P_n(x) \),
\item the generating function \( \sum_{n\ge0}P_n(x)t^n \)
  or \( \sum_{n\ge0}P_n(x)t^n/n! \),
\item the moments \( \{ \mu_n\}_{n\ge 0} \),
\item the 3-term recurrence coefficients \( \{ b_n\}_{n\ge 0} \) and
  \( \{ \lambda_n\}_{n\ge 1} \).
\end{enumerate}

For each OPS, we will show bijectively the equivalence of (3) and (4).

For example, in the case that \( b_k \) and \( \lambda_k \) are
integers, we interprete \( \wt(\alpha) \) as a certain number of
``histories''. To each history we associate bijectively a certain
combinatorial object \( \xi \) of a finite set \( M_n \). Each of
\( b_k \) and \( \lambda_k \) is considered as the number of possible
choices in a stage of a construction of the object, where each stage
corresponds to an elementary step of \( \alpha \). Then it remains to
show that \( |M_n| = \mu_n \).

If \( P_n(x) \) depend on some parameters, it will be sufficient to
consider the histories and the combinatorial objects in \( M_n \).


\subsection{Tchebyshev polynomials}

% \subsection{Hermite polynomials}

% \subsection{Charlier polynomials}

% \subsection{Laguerre polynomials}

% \section{Inverse polynomials}

% \section{Determinants of moments}

% \subsection{Computing the 3-term recurrence coefficients}

% \subsection{Determinants of paths}

% \subsection{Hankel determinants of moments}

% \subsection{The duality between Motzkin paths and Favard paths}

% \subsection{Inverse generating functions}

% \section{Continued fractions}

% \subsection{Development in J-fraction}

% \subsection{Examples}

% \subsection{Convergents}

% \subsection{Symmetric orthogonal polynomials}

% \section{Linearization coefficients}



\newpage

\appendix

\section{Sign-reversing involutions}
\label{sec:sign-revers-invol}

\begin{defn}
  A \emph{sign} of a set \( X \) is a function
  \( \sgn:X \to \{+1,-1\} \). A \emph{sign-reversing involution} on
  \( X \) is an involution \( \phi:X\to X \) such that
  \begin{enumerate}
  \item \( \sgn(x)=1 \) for all \( x\in \Fix(\phi) \);
  \item \( \sgn(\phi(x)) = -\sgn(x) \) for all
  \( x\in X \setminus \Fix(\phi) \),
  \end{enumerate}
  where \( \Fix(\phi) \) is the set of \emph{fixed points} of
  \( \phi \), i.e., \( \Fix(\phi) = \{x\in X: \phi(x) = x \} \).
\end{defn}

It is easy to see that if \( \phi \) is a sign-reversing involution on
\( X \), then
\begin{equation}\label{eq:sgnsum=Fix}
  \sum_{x\in X} \sgn(X) = |\Fix(\phi)|.
\end{equation}

\begin{exam}
  Let's prove the following identity using sign-reversing involutions:
  \begin{equation}\label{eq:4}
    \sum_{k=0}^{n} (-1)^{k} \binom{n}{k} = 0.
  \end{equation}
  To this end we need to construct a set \( X \) and a sign-reversing
  involution \( \phi \) on \( X \) such that \eqref{eq:sgnsum=Fix}
  becomes \eqref{eq:4}.

  Let \( X \) be the set of all subsets of \( [n]:= \{ 1,\dots,n \} \)
  and for \( A\in X \), define \( \sgn(A) = (-1)^{|A|} \). Then it
  suffices to construct a sign-reversing involution on \( X \) with no
  fixed points. This can be done by letting
  \( \phi(A) = A \Delta \{1\} \), where
  \( A \Delta B := (A \cup B) \setminus (A \cap B) \).
\end{exam}


\begin{exam}
  Recall that we proved the following identitiy, which was stated in
  \eqref{eq:charlier-orthogonality}, using generating functions:
  \begin{equation}\label{eq:charlier-orthogonality2}
  \sum_{k\ge 0}  P_m(k) P_n(k) \frac{a^k}{k!} = \frac{e^a a^n}{n!} \delta_{n,m},
  \end{equation}
  where \( P_n(x) \) are the Charlier polynomials defined by
  \[
    P_n(x) = \sum_{k=0}^{n} \binom{x}{k} \frac{(-a)^{n-k}}{(n-k)!}.
  \]
  We will prove this identity using sign-reversing involutions. To do
  this, we will consider \eqref{eq:charlier-orthogonality2} as a power
  series in \( a \). Note that
\begin{align*}
  \sum_{k\ge 0}  P_m(k) P_n(k) \frac{a^k}{k!}
  &=  \sum_{k\ge 0} \sum_{i=0}^m \binom{k}{i} \frac{(-a)^{m-i}}{(m-i)!}
  \sum_{j=0}^n \binom{k}{j} \frac{(-a)^{n-j}}{(n-j)!} \frac{a^k}{k!}\\
  &=  \sum_{k\ge 0} \sum_{i=0}^m \sum_{j=0}^n
    \binom{k}{m-i} \frac{(-a)^{i}}{i!}
   \binom{k}{n-j} \frac{(-a)^{j}}{j!} \frac{a^k}{k!}\\
  &= \sum_{N\ge0} \frac{a^N}{N!} \sum_{i+j+k=N}
  (-1)^{i+j} \frac{N!}{i!j!k!} \binom{k}{m-i} \binom{k}{n-j},
\end{align*}
where \( \binom{r}{s}=0 \) if \( s<0 \).

For a fixed \( N \),
\[
  \sum_{i+j+k=N} (-1)^{i+j} \binom{N}{i,j,k} \binom{k}{m-i} \binom{k}{n-j}
  = \sum_{(A,B,C)\in X} (-1)^{|B\setminus A| + |C\setminus A|},
\]
where \( X \) is the set of triples \( (A,B,C) \) such that
\( A\cup B\cup C = \{ 1,\dots,N \} \), \( |A|=k, |B|=m, |C|=n \),
\( (B\cap C)\setminus A = \emptyset \). Define
\( \sgn(A,B,C) = (-1)^{|B\setminus A| + |C\setminus A|} \). We will
find a sign-reversing involution on \( X \) toggling the smallest
integer in regions \( 1 \) and \( 2 \) or in regions \( 3 \) and
\( 4 \) in Figure~\ref{fig:image1}.
\begin{figure}
  \centering
  \includegraphics[scale=.1]{./figures/image1.jpeg}
  \caption{The triple \( (A,B,C) \).}
  \label{fig:image1}
\end{figure}

To be precise, for \( (A,B,C)\in X \), define \( \phi(A,B,C) \) as
follows.
\begin{description}
\item[Case 1] The regions \( 1,2,3,4 \) are all empty. In this case we define
  \( \phi(A,B,C) = (A,B,C) \).
\item[Case 2] At least one of the regions \( 1,2,3,4 \) is nonempty.
  Let \( s \) be the smallest integer in \( (B\cap C) \setminus A \).
  If \( s \) is in region \( 1 \) (respectively 2, 3, 4), then move
  this integer to region \( 2 \) (respectively 1, 4, 3). Then let
  \( \phi(A,B,C) = (A',B',C') \), where \( A',B',C' \) are the
  resulting sets.
\end{description}

By the construction, \( \phi \) is a sign-reversing involution on
\( X \) whose fixed points are the triples \( (A,B,C) \) such that the
regions \( 1,2,3,4 \) are all empty, that is, \( B=C \subseteq A \).
If \( B=C \subseteq A \), then \( A = [N] \),
so the number of such triples \( (A,B,C) \) is
\( \binom{N}{n} \) if \( m=n \)
and \( 0 \) otherwise.
Thus
\[
  \sum_{(A,B,C)\in X} (-1)^{|B\setminus A| + |C\setminus A|} =
  |\Fix(\phi)| = \delta_{m,n} \binom{N}{n}.
\]
This implies
\[
  \sum_{k\ge 0}  P_m(k) P_n(k) \frac{a^k}{k!}
  = \delta_{m,n} \sum_{N\ge0} \frac{a^N}{N!} \binom{N}{n}
  = \frac{e^a a^n}{n!} \delta_{n,m}.
\]
\end{exam}



\section{Dyck paths and Motzkin paths}
\label{sec:dyck-paths-motzkin}




\section{The Lindstr\"om--Gessel--Viennot lemma}
\label{sec:LGV}



\bibliographystyle{abbrv}
\bibliography{/Users/jangsookim/Library/CloudStorage/Dropbox/newbiemacs/nbm-user-settings/references/ref.bib}

\end{document}
